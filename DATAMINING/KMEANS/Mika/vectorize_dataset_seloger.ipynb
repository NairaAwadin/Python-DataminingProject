{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b18587-8695-434c-a286-32dde24f9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manipulate/organize data/visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#preprocess data\n",
    "import unicodedata as ud\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#Linear regression to fill missing data for certain types\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d2398e11-9286-4bac-853d-cf6e3dcc18f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APPART A VENDR',\n",
       " 'DUPLEX A VENDR',\n",
       " 'LOFT A VENDR',\n",
       " 'MAISON A VENDR',\n",
       " 'MAISON VILL A VENDR',\n",
       " 'STUDIO A VENDR',\n",
       " 'TERRAIN CONSTRUCTIBL A VENDR',\n",
       " 'VILL A VENDR'}"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "03416c4c-6b49-4019-b60b-9d13926d5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEM_TO_POSTCODES = {\n",
    "    \"BOULOGN\": \"92000\",                       # Boulogne-Billancourt\n",
    "    \"BILLANCOURT\": \"92000\",                   # Boulogne-Billancourt\n",
    "    \"BOULOGN BILLANCOURT\": \"92000\",           # Boulogne-Billancourt\n",
    "    \"NEUILLY\": \"92000\",                       # Neuilly-sur-Seine\n",
    "    \"NEUILLY SEIN\": \"92000\",                  # Neuilly-sur-Seine\n",
    "    \"LEVALLOIS PERRET\": \"92000\",              # Levallois-Perret\n",
    "    \"CLICHY\": \"92000\",                        # Clichy\n",
    "    \"CLICHY GAREN\": \"92000\",                  # Clichy-la-Garenne (same CP)\n",
    "    \"SAINT CLOUD\": \"92000\",                   # Saint-Cloud\n",
    "    \"CLOUD\": \"92000\",                         # Saint-Cloud\n",
    "    \"PUTEAU\": \"92000\",                        # Puteaux\n",
    "    \"SURESN\": \"92000\",                        # Suresnes\n",
    "    \"ISSY\": \"92000\",                          # Issy-les-Moulineaux\n",
    "    \"ISSY MOULINEAU\": \"92000\",                # Issy-les-Moulineaux\n",
    "    \"MOULINEAU\": \"92000\",                     # Issy-les-Moulineaux\n",
    "    \"MONTROUG\": \"92000\",                      # Montrouge\n",
    "    \"LE LIL\": \"93000\",                        # Les Lilas\n",
    "    \"AUBERVILLI\": \"93000\",                    # Aubervilliers\n",
    "    \"SAINT OUEN SEIN\": \"93000\",               # Saint-Ouen-sur-Seine\n",
    "    \"OUEN\": \"93000\",                          # Saint-Ouen-sur-Seine\n",
    "    \"CHARENTON\": \"94000\",                     # Charenton-le-Pont\n",
    "    \"CHARENTON PONT\" : \"94000\",\n",
    "    \"VANV\": \"92000\",                          # Vanves\n",
    "    \"MONTREUIL\": \"93000\",                     # Montreuil\n",
    "    \"PANTIN\": \"93000\",                        # Pantin\n",
    "    \"PONT\": \"94000\",                          # (mapped by you to Charenton-le-Pont)\n",
    "    \"SEIN\": \"92000\",                          # (mapped by you to Neuilly-sur-Seine)\n",
    "    \"SAINT DEN\" : \"93000\",\n",
    "    \"DEN\": \"93000\",                           # Saint-Denis\n",
    "    \"PARIS\": \"75000\",\n",
    "    \"BAGNOLET\": \"93000\"\n",
    "}\n",
    "valid_tokens = {'APPART A VENDR',\n",
    "     'AUBERVILLI',\n",
    "     'BAGNOLET',\n",
    "     'BOULOGN BILLANCOURT',\n",
    "     'CHAMBR',\n",
    "     'CHARENTON PONT',\n",
    "     'CLICHY',\n",
    "     'CLICHY GAREN',\n",
    "     'DISPONIBL MAINTEN',\n",
    "     'DIVISIBL',\n",
    "     'DUPLEX A VENDR',\n",
    "     'ETAG',\n",
    "     'HOTEL PARTICULI A VENDR',\n",
    "     'ISSY MOULINEAU',\n",
    "     'LE LIL',\n",
    "     'LEVALLOIS PERRET',\n",
    "     'LOFT A VENDR',\n",
    "     'MAISON A VENDR',\n",
    "     'MAISON VILL A VENDR',\n",
    "     'MONTREUIL',\n",
    "     'MONTROUG',\n",
    "     'MÂ²',\n",
    "     'NEUF',\n",
    "     'NEUILLY SEIN',\n",
    "     'PANTIN',\n",
    "     'PARIS',\n",
    "     'PIEC',\n",
    "     'PUTEAU',\n",
    "     'RDC',\n",
    "     'SAINT CLOUD',\n",
    "     'SAINT DEN',\n",
    "     'SAINT OUEN SEIN',\n",
    "     'STUDIO A VENDR',\n",
    "     'SURESN',\n",
    "     'TERRAIN',\n",
    "     'TERRAIN CONSTRUCTIBL A VENDR',\n",
    "     'VANV',\n",
    "     'VILL A VENDR'}\n",
    "valid_types = {\n",
    "    'APPART A VENDR',\n",
    "    'DUPLEX A VENDR',\n",
    "    'LOFT A VENDR',\n",
    "    'MAISON A VENDR',\n",
    "    'MAISON VILL A VENDR',\n",
    "    'STUDIO A VENDR',\n",
    "    'TERRAIN CONSTRUCTIBL A VENDR',\n",
    "    'VILL A VENDR'\n",
    "}\n",
    "\n",
    "# Currency tokens (symbol or word forms)\n",
    "EUR_PATTERN = re.compile(\n",
    "    r\"(â‚¬|\\b(?:eur|euro(?:s)?)\\b)\", re.IGNORECASE\n",
    ")\n",
    "# âœ… Matches: \"â‚¬\", \"EUR\", \"euro\", \"euros\", \"Prix: 200 eur\"\n",
    "# âŒ Doesnâ€™t: \"eurostar\" (word boundary blocks it), \"EUROPE\" (not 'euro' exactly)\n",
    "# âš ï¸ Note: Wonâ€™t catch \"â‚¬1,200\" as a wholeâ€”this only finds the currency token, not the number.\n",
    "\n",
    "GBP_PATTERN = re.compile(\n",
    "    r\"(Â£|\\b(?:gbp|pound(?:s)?)\\b)\", re.IGNORECASE\n",
    ")\n",
    "# âœ… Matches: \"Â£\", \"GBP\", \"pound\", \"pounds\", \"200 gbp\"\n",
    "# âŒ Doesnâ€™t: \"pounding\", \"compound\" (word boundary prevents false positives)\n",
    "# âš ï¸ \"$\" in \"CA$\" is not relevant here; thatâ€™s for USD_PATTERN.\n",
    "\n",
    "USD_PATTERN = re.compile(\n",
    "    r\"(\\$|\\b(?:usd|dollar(?:s)?)\\b)\", re.IGNORECASE\n",
    ")\n",
    "# âœ… Matches: \"$\", \"USD\", \"dollar\", \"dollars\", \"2 usd\"\n",
    "# âŒ Doesnâ€™t: \"sandollar\" (needs word boundary), other currency symbols like \"C$\", \"A$\"\n",
    "# âš ï¸ \"$\" is ambiguous across countries; consider context if you need only US dollars.\n",
    "\n",
    "# General number with optional decimal (dot or comma)\n",
    "NUM_PATTERN = re.compile(r\"\\d+(?:[.,]\\d+)?\")\n",
    "# âœ… Matches: \"5\", \"12\", \"83.5\", \"1,75\", \"0003\"\n",
    "# âŒ Doesnâ€™t: signed numbers like \"-3.2\" (see PATTERN_NUM), fractions like \"1/7\" (you said slash handled separately)\n",
    "# âš ï¸ Will also match date parts (\"12/10\") individually.\n",
    "\n",
    "# French ordinal like \"1ER\", \"12EM\"\n",
    "ORDINAL_PATTERN = re.compile(r\"\\b\\d+(?:ER|EM)\\b\", flags=re.IGNORECASE)\n",
    "# âœ… Matches: \"1ER\", \"2er\", \"12EM\", \"3em\"\n",
    "# âŒ Doesnâ€™t: \"1ÃˆRE\", \"2ÃˆME\" (accented forms), \"1st\", \"2nd\"\n",
    "# âš ï¸ If you need accented French ordinals, expand to include ÃˆRE/ÃˆME variants.\n",
    "\n",
    "# Exact \"PARIS\" (uppercase only)\n",
    "PARIS_PATTERN = re.compile(r\"PARIS\")\n",
    "# âœ… Matches: \"PARIS\"\n",
    "# âŒ Doesnâ€™t: \"Paris\", \"paris\" (case-sensitive), arrondissements like \"75015 Paris\" unless uppercased\n",
    "# ðŸ’¡ Consider re.compile(r\"\\bparis\\b\", re.I) if you want case-insensitive.\n",
    "\n",
    "# Signed integer/float with dot decimal\n",
    "PATTERN_NUM = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n",
    "# âœ… Matches: \"-12\", \"+3.5\", \"0.99\"\n",
    "# âŒ Doesnâ€™t: comma decimals (\"1,75\"), fractions (\"1/7\")\n",
    "# âš ï¸ This one is stricter than NUM_PATTERN (only dot decimals, allows sign).\n",
    "\n",
    "# Area cues: \"mÂ²\", \"m2\", \"surfaceâ€¦\", or \"terrain\"\n",
    "PATTERN_AREA = re.compile(r\"(?:[-â€“â€”]?\\s*)(?:M(?:Â²|2)|SURFAC\\w*|TERRAIN)\", re.I)\n",
    "# âœ… Matches: \"mÂ²\", \"M2\", \"surface\", \"surfaces\", \"terrain\", with optional preceding dash/space\n",
    "# âŒ Doesnâ€™t: \"m^2\" (caret), \"area\" (English), \"superficie\" (if that term appears, itâ€™s missed)\n",
    "# âš ï¸ \"TERRAIN\" here might be too broad if you treat lots vs. living area differently.\n",
    "\n",
    "# Standalone \"TERRAIN\"\n",
    "PATTERN_TERRAIN = re.compile(r\"\\bTERRAIN\\b\", re.I)\n",
    "# âœ… Matches only the word \"terrain\" as a token\n",
    "# âŒ Doesnâ€™t: \"terrainment\", \"souterrain\" (goodâ€”no false positives)\n",
    "\n",
    "# Any digit anywhere\n",
    "PATTERN_HAS_DIGIT = re.compile(r\"\\d\")\n",
    "# âœ… Matches if string contains at least one digit\n",
    "# âŒ Doesnâ€™t: strings with numbers written as words (\"trois\", \"three\")\n",
    "\n",
    "# Availability \"disponible maintenant\" with flexible endings\n",
    "PATTERN_DISPO_NOW = re.compile(r\"\\bDISPONIBL\\w*\\s+MAINTEN\\w*\\b\", re.I)\n",
    "# âœ… Matches: \"disponible maintenant\", \"DisponibilitÃ© maintenue\" (careful), \"disponibles maintienâ€¦\"\n",
    "# âŒ Doesnâ€™t: rearranged word order (\"maintenant disponible\"), abbreviations\n",
    "# âš ï¸ Broad \\w* can overmatch (\"maintenue\"); tighten if needed to specifically \"disponible maintenant\".\n",
    "\n",
    "# Floor indicators: \"Ã©tage\" or \"RDC\" (ground floor)\n",
    "PATTERN_FLOOR = re.compile(r\"\\b(?:ETAG\\w*|RDC)\\b\", re.I)\n",
    "# âœ… Matches: \"etage\", \"Ã©tages\", \"RDC\"\n",
    "# âŒ Doesnâ€™t: \"Rez-de-chaussÃ©e\" written fully (unless you rely on \"RDC\")\n",
    "# âš ï¸ You also have dedicated ETAG/RDC patterns below; avoid double work.\n",
    "    \n",
    "PATTERN_ETAG = re.compile(r\"\\bETAG\\w*\\b\", re.I)\n",
    "# âœ… Matches: \"etage\", \"Ã©tages\", \"Ã©tagÃ©\" (could be false positive)\n",
    "# âŒ Doesnâ€™t: \"Rez-de-chaussÃ©e\", \"R+1\" (common shorthand not covered)\n",
    "\n",
    "PATTERN_RDC = re.compile(r\"\\bRDC\\w*\\b\", re.I)\n",
    "# âœ… Matches: \"RDC\", \"RDC+1\" (because \\w*), \"RDC.\"\n",
    "# âŒ Doesnâ€™t: \"Rez de chaussÃ©e\" written without \"RDC\"\n",
    "# âš ï¸ If you donâ€™t want \"RDC+1\", use r\"\\bRDC\\b\".\n",
    "\n",
    "# Rooms / pieces\n",
    "PATTERN_PIECE = re.compile(r\"\\bPIEC\\w*\\b\", re.I)\n",
    "# âœ… Matches: \"piece\", \"piÃ¨ces\", \"piÃ¨ce(s)\"\n",
    "# âŒ Doesnâ€™t: abbreviations like \"T2\", \"F3\" (common in FR real-estate)\n",
    "\n",
    "# New / newly built\n",
    "PATTERN_NEUF = re.compile(r\"\\bNEUF\\w*\\b\", re.I)\n",
    "# âœ… Matches: \"neuf\", \"neuve\", \"neufs\", \"neuves\"\n",
    "# âŒ Doesnâ€™t: \"neuf\" meaning the number 9 when used numerically (context needed)\n",
    "# âš ï¸ Ambiguous: \"neuf\" can be adjective \"new\" or the noun/adjective \"nine\".\n",
    "\n",
    "# Bedrooms\n",
    "PATTERN_CHAMBRE = re.compile(r\"\\bCHAMBR\\w*\\b\", re.I)\n",
    "# âœ… Matches: \"chambre\", \"chambres\"\n",
    "# âŒ Doesnâ€™t: abbreviations (\"chb\", \"CH.\"), \"suite parentale\" (no keyword \"chambre\")\n",
    "\n",
    "# Number BEFORE a slash (capture group = the number)\n",
    "PATTERN_BEFORE_SLASH = re.compile(r\"([+-]?\\d+(?:\\.\\d+)?)\\s*/\")\n",
    "# âœ… Matches the \"12\" in \"12 / 5\", \"+3.5/7\"\n",
    "# âŒ Doesnâ€™t: \"12,5/7\" (comma decimals), no slash present\n",
    "# âš ï¸ Will also match dates (\"12/10\")â€”disambiguate upstream if needed.\n",
    "\n",
    "# Number AFTER a slash (capture group = the number)\n",
    "PATTERN_AFTER_SLASH = re.compile(r\"/\\s*([+-]?\\d+(?:\\.\\d+)?)\")\n",
    "# âœ… Matches the \"5\" in \"12 / 5\", the \"7\" in \"3.5/7\"\n",
    "# âŒ Doesnâ€™t: \"12/1,5\" (comma decimals)\n",
    "\n",
    "# Literal slash (useful for quick checks/splits)\n",
    "PATTERN_SLASH = re.compile(r\"/\")\n",
    "# âœ… Matches any \"/\" character\n",
    "# âŒ Doesnâ€™t: division written as \"Ã·\" or \"per\" words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc2d090a-3578-4207-bf17-b6c29f3ec55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text: str):\n",
    "    #replace diacritics\n",
    "    text = (text.replace(\"Å“\", \"oe\").replace(\"Å’\", \"OE\")\n",
    "           .replace(\"Ã¦\", \"ae\").replace(\"Ã†\", \"AE\"))\n",
    "    #decompose form 'Ã¨' -> 'e' + '`'\n",
    "    text = ud.normalize(\"NFD\",text)\n",
    "    #remove accents like '`','^',...\n",
    "    text = \"\".join([char for char in text if ud.category(char) != \"Mn\"])\n",
    "    #recompose \n",
    "    text = ud.normalize(\"NFC\",text)\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08354033-c66e-45c9-b63f-3dbd04b6f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_french_nltk(text: str):\n",
    "    text = remove_accents(text)\n",
    "    stemmer = SnowballStemmer(\"french\")\n",
    "    stop_fr = set(stopwords.words(\"french\"))\n",
    "    tkns = re.findall(r\"[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿\\d/(m2|mÂ²).]+\", text)\n",
    "    tkns = [t for t in tkns if t not in stop_fr]\n",
    "    stems  = [stemmer.stem(t) for t in tkns]\n",
    "    return (\" \".join(stems)).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0940405-90df-41c4-9b42-f4a4e95e009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(text : str = \"\"):\n",
    "    #convert \",\" of float num to \".\"\n",
    "    text = re.sub(r\"(?<=\\d),(?=\\d)\",\".\",text)\n",
    "    #convert \"-\" to \" \"\n",
    "    text = re.sub(r'(?<=[^\\W\\d_])-(?=[^\\W\\d_])', ' ', text, flags=re.UNICODE)\n",
    "    tkns = [tkn.strip() for tkn in re.split(\"[-,]\",text)]\n",
    "    return tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dc7e60b-723a-4d86-90e7-69f05861c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_2price(price_str : str = None)->str:\n",
    "    try :\n",
    "        price_str = \"\".join([chr for chr in price_str if chr.isdigit()])\n",
    "        return float(price_str)\n",
    "    except Exception :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "502f613f-c070-45c5-adf6-270437fd9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(tkns : list = None):\n",
    "    if not tkns :\n",
    "        return None\n",
    "    EUR_PATTERN = re.compile(r\"(â‚¬|\\b(?:eur|euro(?:s)?)\\b)\", re.IGNORECASE)\n",
    "    GBP_PATTERN = re.compile(r\"(Â£|\\b(?:gbp|pound(?:s)?)\\b)\", re.IGNORECASE)\n",
    "    USD_PATTERN = re.compile(r\"(\\$|\\b(?:usd|dollar(?:s)?)\\b)\", re.IGNORECASE)\n",
    "    m_tkns = []\n",
    "    prices = []\n",
    "    if not tkns :\n",
    "        raise ValueError(\"elements = None\")\n",
    "    for tkn in tkns :\n",
    "        if bool(EUR_PATTERN.search(tkn)):\n",
    "            prices.append([tkn,\"EUR\"])\n",
    "        elif bool(GBP_PATTERN.search(tkn)):\n",
    "            prices.append([tkn,\"GBP\"])\n",
    "        elif bool(USD_PATTERN.search(tkn)):\n",
    "            prices.append([tkn,\"USD\"])\n",
    "        else :\n",
    "            m_tkns.append(tkn)\n",
    "    if len(prices) != 1:\n",
    "        raise ValueError(\"More than 1 or no price for this property\")\n",
    "    return [m_tkns,prices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f87e07d0-6e05-437d-a556-8095a3b24149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed(raw_df):\n",
    "    #df = pd.DataFrame(columns=[\"_\",\"PRICE\",\"UNIT-PRICE\"])\n",
    "    list_tags = []\n",
    "    prices =[]\n",
    "    price_units = []\n",
    "    raws = []\n",
    "    for i in range(len(raw_df)):\n",
    "        tags,(price_str,price_unit)=extract_price(extract_tokens(raw_df.iloc[i]))\n",
    "        tags = list(map(preprocess_french_nltk,tags))\n",
    "        price = convert_2price(price_str)\n",
    "        if price == None :\n",
    "            print(price_str)\n",
    "        if price > 1000 :# add threshold to prevent false pricing\n",
    "            list_tags.append(tags)\n",
    "            prices.append(price)\n",
    "            price_units.append(price_unit)\n",
    "            raws.append(raw_df.iloc[i])\n",
    "    return (pd.DataFrame({\"RAW\" : tuple(raws),\"DATA TAG\":list_tags,\"PRICE\":prices,\"PRICE_UNIT\":price_units}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "944059f4-2ed6-4568-8a1e-35f5639e37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_token(tag):\n",
    "    tag = tag.upper().strip()\n",
    "    if \"ETAG\" in tag :\n",
    "        return \"ETAG\"\n",
    "    if 'RDC' in tag:    \n",
    "        return 'RDC'\n",
    "    \"\"\"\n",
    "    if 'TERRAIN' in tag:\n",
    "        return 'TERRAIN'\n",
    "    \"\"\"\n",
    "    if 'SURFAC' in tag:\n",
    "        return 'SURFAC'\n",
    "    #remove tags\n",
    "    if 'DIVISIBL A PART MÂ²' in tag or 'DIVISIBL JUSQU A MÂ²' in tag or 'DIVISIBL MÂ² A MÂ²' in tag:\n",
    "        return None\n",
    "    #reduce all MÂ² to MÂ²\n",
    "    if 'MÂ²' in tag :\n",
    "        return 'MÂ²'\n",
    "\n",
    "    # Remove ordinals like 1ER / 12EM and plain numbers/decimals\n",
    "    tag = ORDINAL_PATTERN.sub('', tag)\n",
    "    tag = tag.replace('/', ' ')            # split fractions like \"1/7\"\n",
    "    tag = NUM_PATTERN.sub('', tag)\n",
    "\n",
    "    # Collapse whitespace\n",
    "    tag = re.sub(r'\\s+', ' ', tag).strip()\n",
    "\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "aa418fe8-5ad3-4df4-b81d-b8ae411dec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tags(tags):\n",
    "    n_tkns = [] #normalized tokens\n",
    "    for tkn in tags :\n",
    "        if str(tkn).strip() :\n",
    "            n_tkn = normalize_token(tkn)\n",
    "            if n_tkn:\n",
    "                if n_tkn in valid_tokens :\n",
    "                    n_tkns.append(n_tkn)\n",
    "                else :\n",
    "                    return pd.NA\n",
    "    if len(n_tkns) == 0 :\n",
    "        return None\n",
    "    return tuple(n_tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c39f5923-6439-4391-ad0e-bf5b9c4b1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(seqs):\n",
    "    return {normalize_sequence(tags) for tags in seqs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "56786a4d-4f01-41c5-8981-a114c833220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_type(cleaned_tags):\n",
    "    for t in cleaned_tags :\n",
    "        if t in valid_types :\n",
    "            return t\n",
    "    return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85881c09-e53f-4f67-be11-4ffefbbee8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paris_cp(tags):\n",
    "    paris_tag = \"\"\n",
    "    for tag in tags :\n",
    "        if \"PARIS\" in tag :\n",
    "            if not bool(re.search(r\"\\d\", tag))  :\n",
    "                print(\"Got tag with no digit : \", tag)\n",
    "                return None\n",
    "            try :\n",
    "                digits = int(\"\".join([ch for ch in tag if ch.isdigit()]))\n",
    "                if digits <= 20 and digits > 0:\n",
    "                    return str(75000 + int(digits))\n",
    "                else :\n",
    "                    print(\"District code doesn't exist\")\n",
    "                    return None\n",
    "            except Exception :\n",
    "                print(f\"Error encountered (got digits = {digits}): \", tag)\n",
    "                return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "997807a3-8f8e-46bd-961d-dc88528598f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's extract all locations to a column\n",
    "def extract_loc(df,dict_map):\n",
    "    \"\"\"\n",
    "    extract_loc is gonna focus on inside paris, and others will be more generalized \n",
    "    \"\"\"\n",
    "    df= df.copy()\n",
    "    mask_not_paris = df[\"CLEANED TAG\"].apply(lambda x: isinstance(x, (list, tuple, set)) and \"PARIS\" not in x)\n",
    "    \n",
    "    valid_locs = set(dict_map.keys())\n",
    "    df.loc[mask_not_paris, \"LOC\"] = (\n",
    "        df.loc[mask_not_paris, \"CLEANED TAG\"]\n",
    "          .apply(lambda x: (set(x) & valid_locs) if x else pd.NA)\n",
    "          .apply(lambda x: {dict_map[c] for c in x} if x else pd.NA)\n",
    "    )\n",
    "    \n",
    "    df.loc[~mask_not_paris,\"LOC\"] = (\n",
    "        df[~mask_not_paris][\"DATA TAG\"].apply(lambda x : set([get_paris_cp(x)]) if get_paris_cp(x) else pd.NA)\n",
    "    )\n",
    "    \n",
    "    mask_multi_loc = df[\"LOC\"].apply(lambda x: isinstance(x, (list, tuple, set)) and len(x) > 1)\n",
    "\n",
    "    #print(\"Multi loc assigned detected : \",(mask_multi_loc & mask_not_paris).sum())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4fbb776d-5a73-45b8-8185-b074dea9a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numbers_in_text(s):\n",
    "    \"\"\"Return all numbers (as floats) found in a string. Spaces already preprocessed upstream.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s_clean = s.replace(\" \", \"\")  # e.g. '2 645' -> '2645'\n",
    "    return [float(m.group(0)) for m in PATTERN_NUM.finditer(s_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "57a37631-3171-4e8d-afba-a58b1171203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To normalize surface area,\n",
    "we decide to bring all to a singular scalar value for all type of surface except TERRAIN, which will be kept as 2 scalars.\n",
    "-> All surface area (except \"TERRAIN\") will be treated in the same \"area\" dimension.\n",
    "-> for Bureau surface area, we will take the smallest among all displayed area. \n",
    "(why? because advertised price often represent the minimum value product)\n",
    "-> for pair like \"m2\" and \"TERRAIN\" will be treated as 2 scalars.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Now let's extract area, we know that area can be represented in 3 tags : \n",
    "-MÂ²\n",
    "-SURFAC\n",
    "-TERRAIN\n",
    "\"\"\"\n",
    "\n",
    "def extract_area(df):\n",
    "    \"\"\"\n",
    "    Normalize surface data into two scalars per row:\n",
    "      - AREA: one scalar for all non-TERRAIN surfaces (or TERRAIN tokens without digits)\n",
    "      - TERRAIN: one scalar for terrain (only if the token contains 'TERRAIN' + a digit)\n",
    "\n",
    "    Inputs:\n",
    "      df         : DataFrame\n",
    "      col_seq    : column with a token list/tuple/set per row (used to detect presence of area tags)\n",
    "      col_tag    : column with a token list/tuple/set per row (we extract numbers from here)\n",
    "      area_tags  : {'MÂ²','SURFAC','TERRAIN'}\n",
    "      area_pattern: compiled regex to recognize any of MÂ² / SURFAC(e) / TERRAIN tokens\n",
    "\n",
    "    Returns:\n",
    "      DataFrame copy with columns:\n",
    "        - 'ALL AREA' : list of tokens recognized as area-related in col_tag\n",
    "        - 'AREA'     : float (or <NA>)\n",
    "        - 'TERRAIN'  : float (or <NA>)\n",
    "    \"\"\"\n",
    "    area_tags = {\"MÂ²\",\"SURFAC\",\"TERRAIN\"}\n",
    "    has_area = df[\"CLEANED TAG\"].apply(\n",
    "        lambda xs: bool(set(xs) & set(area_tags)) if isinstance(xs, (list, tuple, set)) else False\n",
    "    )\n",
    "    out = df.loc[has_area].copy()\n",
    "    def get_area_tokens(tokens):\n",
    "        if not isinstance(tokens, (list, tuple, set)):\n",
    "            return pd.NA\n",
    "        hits = []\n",
    "        for t in tokens:\n",
    "            if re.search(PATTERN_AREA, str(t)):\n",
    "                hits.append(str(t))\n",
    "        if hits:\n",
    "            return hits\n",
    "        return pd.NA\n",
    "\n",
    "    out[\"ALL AREA\"] = out[\"DATA TAG\"].apply(\n",
    "        lambda xs: get_area_tokens(list(xs)) if isinstance(xs, (list, tuple, set)) else pd.NA\n",
    "    )\n",
    "    def get_lowest_value(tokens):\n",
    "        if not tokens:\n",
    "            return pd.NA\n",
    "        vals = []\n",
    "        for t in tokens:\n",
    "            try:\n",
    "                vals.extend(_numbers_in_text(t))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return min(vals) if vals else pd.NA\n",
    "\n",
    "    def is_divisible(tokens):\n",
    "        if not tokens:\n",
    "            return pd.NA\n",
    "        for t in tokens:\n",
    "            if isinstance(t, str) and \"DIVISIBL\" in t:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def split_area_vs_terrain(tokens):\n",
    "        if not isinstance(tokens, (list, tuple, set)):\n",
    "            return [], []\n",
    "        area_tokens, terrain_tokens = [], []\n",
    "        for t in tokens:\n",
    "            t_str = str(t)\n",
    "            has_terrain = bool(PATTERN_TERRAIN.search(t_str))\n",
    "            has_digit   = bool(PATTERN_HAS_DIGIT.search(t_str))\n",
    "            if has_terrain and has_digit:\n",
    "                terrain_tokens.append(t_str)\n",
    "            else:\n",
    "                area_tokens.append(t_str)\n",
    "        return area_tokens, terrain_tokens\n",
    "\n",
    "    # 3) compute scalars for AREA and TERRAIN from 'ALL AREA'\n",
    "    def compute_scalars(tokens):\n",
    "        if not isinstance(tokens, (list, tuple, set)) or not tokens:\n",
    "            return pd.Series({\"AREA\": pd.NA, \"TERRAIN\": pd.NA})\n",
    "\n",
    "        area_tokens, terrain_tokens = split_area_vs_terrain(tokens)\n",
    "        if area_tokens:\n",
    "            if isinstance(is_divisible(area_tokens), bool) and is_divisible(area_tokens):\n",
    "                area_val = get_lowest_value(area_tokens)\n",
    "            else:\n",
    "                area_val = get_lowest_value(area_tokens)\n",
    "        else:\n",
    "            area_val = pd.NA\n",
    "        terrain_val = get_lowest_value(terrain_tokens) if terrain_tokens else pd.NA\n",
    "\n",
    "        return pd.Series({\"AREA\": area_val, \"TERRAIN\": terrain_val})\n",
    "\n",
    "    scalars = out[\"ALL AREA\"].apply(compute_scalars)\n",
    "    out.loc[:, \"AREA\"] = scalars[\"AREA\"]\n",
    "    out.loc[:, \"TERRAIN\"] = scalars[\"TERRAIN\"]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "68113445-d735-4d7e-b7fc-c250ad8d99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool tag detect function\n",
    "def is_dispo(tags):\n",
    "    # DISPONIBL MAINTEN: 1 if present, 0 if explicitly absent, <NA> if no tokens\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_DISPO_NOW.search(str(t)) for t in tags) else 0\n",
    "\n",
    "def is_neuf(tags):\n",
    "    # NEUF: 1 if present, 0 if explicitly absent, <NA> if no tokens\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_NEUF.search(str(t)) for t in tags) else 0\n",
    "\n",
    "def has_chambr(tags):\n",
    "    # CHAMBR: 1 if present, <NA> otherwise (to be averaged later, per your design)\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_CHAMBRE.search(str(t)) for t in tags) else pd.NA\n",
    "\n",
    "def has_piece(tags):\n",
    "    # PIEC: 1 if present, <NA> otherwise (to be averaged later)\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_PIECE.search(str(t)) for t in tags) else pd.NA\n",
    "\n",
    "def has_floor(tags):\n",
    "    # ETAG/RDC: 1 if present, 0 if explicitly absent, <NA> if no tokens\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_FLOOR.search(str(t)) for t in tags) else 0\n",
    "\n",
    "#bool for floor extraction\n",
    "def is_rdc(tag):\n",
    "    if not tag :\n",
    "        raise ValueError(\"No tag provided\")\n",
    "    if bool(PATTERN_RDC.search(tag)):\n",
    "        return 1\n",
    "    else : return 0\n",
    "def is_etag(tag):\n",
    "    if not tag :\n",
    "        raise ValueError(\"No tag provided\")\n",
    "    if bool(PATTERN_ETAG.search(tag)):\n",
    "        return 1\n",
    "    else : return 0    \n",
    "def has_slash(tag):\n",
    "    if not tag:\n",
    "        raise ValueError(\"No tag provided\")\n",
    "    if bool(PATTERN_SLASH.search(tag)):\n",
    "        return 1\n",
    "    else : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cd0f92a9-3613-44e9-9627-6bbbd94e4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_additionals(df,col_seq=\"CLEANED TAG\",col_tag=\"DATA TAG\"):\n",
    "    #create masks\n",
    "    mask_is_dispo = df[col_seq].apply(lambda x : is_dispo(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_is_neuf = df[col_seq].apply(lambda x : is_neuf(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_has_chambr = df[col_seq].apply(lambda x : has_chambr(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_has_piece = df[col_seq].apply(lambda x : has_piece(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_has_floor = df[col_seq].apply(lambda x : has_floor(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    #extract tags\n",
    "    def get_chambr_tag(tags):\n",
    "        if not tags :\n",
    "            raise ValueError(\"Tags not provided\")\n",
    "        if not isinstance(tags,(set,list,tuple)):\n",
    "            return pd.NA\n",
    "        for t in tags :\n",
    "            if bool(PATTERN_CHAMBRE.search(t)):\n",
    "                return t\n",
    "        return pd.NA\n",
    "    def get_piece_tag(tags):\n",
    "        if not tags :\n",
    "            raise ValueError(\"Tags not provided\")\n",
    "        if not isinstance(tags,(set,list,tuple)):\n",
    "            return pd.NA\n",
    "        for t in tags :\n",
    "            if bool(PATTERN_PIECE.search(t)):\n",
    "                return t\n",
    "        return pd.NA\n",
    "    def get_floor_tag(tags):\n",
    "        if not tags :\n",
    "            raise ValueError(\"Tags not provided\")\n",
    "        if not isinstance(tags,(set,list,tuple)):\n",
    "            return pd.NA\n",
    "        for t in tags :\n",
    "            if bool(PATTERN_FLOOR.search(t)):\n",
    "                return t\n",
    "        return pd.NA\n",
    "\n",
    "    #get numerical values\n",
    "    def get_floor(tag):\n",
    "        if not tag :\n",
    "            raise ValueError(\"No tag provided\")\n",
    "        nb_floor = PATTERN_BEFORE_SLASH.search(tag)\n",
    "        if bool(nb_floor):\n",
    "            return int(float(nb_floor.group(1)))\n",
    "        else : return 0\n",
    "    def get_floors(tag):\n",
    "        if not tag :\n",
    "            raise ValueError(\"No tag provided\")\n",
    "        nb_floors = PATTERN_AFTER_SLASH.search(tag)\n",
    "        if bool(nb_floors):\n",
    "            return int(float(nb_floors.group(1)))\n",
    "        else : return 0\n",
    "    def get_num(tag):\n",
    "        tag = str(tag)\n",
    "        if not tag :\n",
    "            raise ValueError(\"tag not provided\")\n",
    "        num = PATTERN_NUM.search(tag)\n",
    "        if bool(num):\n",
    "            return int(num.group(0))\n",
    "        else :\n",
    "            return 0\n",
    "    def split_floor_tag(tag):\n",
    "        if not tag or not bool(PATTERN_FLOOR.search(tag)):\n",
    "            raise ValueError(\"tag not provided or no floor pat in tag\")\n",
    "    \n",
    "        is_rdc_flag = bool(PATTERN_RDC.search(tag))\n",
    "        has_slash   = bool(PATTERN_SLASH.search(tag))\n",
    "    \n",
    "        floor, floors = 0, 0\n",
    "        if is_rdc_flag:\n",
    "            floor = 0\n",
    "        if has_slash:\n",
    "            nb_floor  = PATTERN_BEFORE_SLASH.search(tag)\n",
    "            nb_floors = PATTERN_AFTER_SLASH.search(tag)\n",
    "            if not is_rdc_flag and nb_floor:\n",
    "                floor = int(float(nb_floor.group(1)))\n",
    "            if nb_floors:\n",
    "                floors = int(float(nb_floors.group(1)))\n",
    "            if not nb_floors:\n",
    "                floors = floor\n",
    "        else:\n",
    "            if not is_rdc_flag:\n",
    "                floor = int(get_num(tag)) or 0\n",
    "            floors = floor\n",
    "    \n",
    "        return (floor, floors)\n",
    "\n",
    "    #bool extraction\n",
    "    series_dispo = df[col_seq].apply(lambda x : is_dispo(x))\n",
    "    series_neuf = df[col_seq].apply(lambda x : is_neuf(x))\n",
    "\n",
    "    #unconditional extraction & pd.NA to be averaged\n",
    "    #chambre\n",
    "    series_chambr = df[col_seq].apply(lambda x : has_chambr(x))\n",
    "    series_chambr.loc[series_chambr.notna()] = df.loc[mask_has_chambr,col_tag].apply(lambda x : get_chambr_tag(x))\n",
    "    series_chambr.loc[series_chambr.notna()] = series_chambr.loc[series_chambr.notna()].apply(lambda x : int(get_num(x)) if x else pd.NA)\n",
    "    is_valid_series_chambr = series_chambr.dropna().apply(lambda x : isinstance(x,int)).all()\n",
    "    if not is_valid_series_chambr :\n",
    "        raise ValueError(\"is_valid_series_chambr not valid\")\n",
    "    #piece\n",
    "    series_piece = df[col_seq].apply(lambda x : has_piece(x))\n",
    "    series_piece.loc[series_piece.notna()] = df.loc[mask_has_piece,col_tag].apply(lambda x : get_piece_tag(x))    \n",
    "    series_piece.loc[series_piece.notna()] = series_piece.loc[series_piece.notna()].apply(lambda x : int(get_num(x)) if x else pd.NA)\n",
    "    is_valid_series_piece = series_piece.dropna().apply(lambda x : isinstance(x,int)).all()\n",
    "    if not is_valid_series_piece :\n",
    "        raise ValueError(\"is_valid_series_piece not valid\")\n",
    "    #conditional extraction\n",
    "    series_floor_tag = df[col_seq].apply(lambda x: has_floor(x))\n",
    "    mask_floor_any = series_floor_tag.fillna(0).astype(bool)\n",
    "    series_floor_tag.loc[mask_floor_any] = df.loc[mask_has_floor, col_tag].apply(lambda x: get_floor_tag(x) if x else pd.NA)\n",
    "    series_floor_tag.loc[mask_floor_any] = series_floor_tag.loc[mask_floor_any].apply(lambda x: split_floor_tag(x) if isinstance(x, str) else (0, 0))\n",
    "    idx = ~mask_floor_any\n",
    "    series_floor_tag.loc[idx] = [(0, 0)] * int(idx.sum())\n",
    "\n",
    "    \n",
    "    # validate\n",
    "    is_valid_series_floor = series_floor_tag.apply(lambda x: isinstance(x, tuple) and len(x) == 2).all()\n",
    "\n",
    "    if not is_valid_series_floor :\n",
    "        raise ValueError(\"is_valid_series_floor not valid\")\n",
    "    if len(series_floor_tag) == len(series_chambr) == len(series_piece) == len(series_dispo) == len(series_neuf) :\n",
    "        out = df.copy(deep=True)\n",
    "        out[\"DISPONIBL MAINTEN\"] = series_dispo\n",
    "        out[\"NEUF\"] = series_neuf\n",
    "        out[\"CHAMBRE\"] = series_chambr\n",
    "        out[\"PIECE\"] = series_piece\n",
    "        out[[\"FLOOR\",\"FLOORS\"]] = pd.DataFrame(series_floor_tag.tolist(), index=df.index)\n",
    "        return out\n",
    "    else : \n",
    "        raise ValueError(\"len don't match\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "977af84d-bd1e-40bb-964d-dd68c7a56c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def round_int(arr):\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    return np.round(a).astype(int)\n",
    "\n",
    "def as_float(arr):\n",
    "    return np.asarray(arr, dtype=float)\n",
    "\n",
    "def make_model(x_cols, alpha=1.0):\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\", StandardScaler()),\n",
    "        ]), x_cols)\n",
    "    ])\n",
    "    return Pipeline([(\"pre\", pre), (\"reg\", Ridge(alpha=alpha))])\n",
    "\n",
    "# ---------- train three models (Option B) ----------\n",
    "def train_option_b(df, alpha_piece=1.0, alpha_ch_direct=1.0, alpha_ch_chain=1.0):\n",
    "    df = df.copy()\n",
    "    for c in [\"AREA\", \"PIECE\", \"CHAMBRE\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df_piece    = df.dropna(subset=[\"PIECE\"])\n",
    "    df_ch_dir   = df.dropna(subset=[\"CHAMBRE\"])\n",
    "    df_ch_chain = df.dropna(subset=[\"CHAMBRE\", \"PIECE\"])\n",
    "\n",
    "    m_piece_from_area = make_model([\"AREA\"], alpha=alpha_piece)\n",
    "    m_piece_from_area.fit(df_piece[[\"AREA\"]], df_piece[\"PIECE\"])\n",
    "\n",
    "    m_chambre_from_area = make_model([\"AREA\"], alpha=alpha_ch_direct)\n",
    "    m_chambre_from_area.fit(df_ch_dir[[\"AREA\"]], df_ch_dir[\"CHAMBRE\"])\n",
    "\n",
    "    m_chambre_from_area_piece = make_model([\"AREA\", \"PIECE\"], alpha=alpha_ch_chain)\n",
    "    m_chambre_from_area_piece.fit(df_ch_chain[[\"AREA\", \"PIECE\"]], df_ch_chain[\"CHAMBRE\"])\n",
    "\n",
    "    return {\n",
    "        \"piece_from_area\": m_piece_from_area,\n",
    "        \"chambre_from_area\": m_chambre_from_area,\n",
    "        \"chambre_from_area_piece\": m_chambre_from_area_piece,\n",
    "    }\n",
    "\n",
    "# ---------- predict from AREA only ----------\n",
    "def predict_from_area(models, area_value: float):\n",
    "    X_area = pd.DataFrame([{\"AREA\": float(area_value)}])\n",
    "\n",
    "    # piece <- area\n",
    "    piece_hat = float(models[\"piece_from_area\"].predict(X_area)[0])\n",
    "    piece_hat = max(1.0, piece_hat)\n",
    "    piece_hat_round = int(round(piece_hat))\n",
    "\n",
    "    # chambre direct <- area\n",
    "    chambre_direct = float(models[\"chambre_from_area\"].predict(X_area)[0])\n",
    "    chambre_direct = max(0.0, chambre_direct)\n",
    "    chambre_direct_round = int(round(min(piece_hat_round, chambre_direct)))\n",
    "\n",
    "    # chambre chained <- (area, pieceÌ‚)\n",
    "    X_chain = pd.DataFrame([{\"AREA\": float(area_value), \"PIECE\": piece_hat}])\n",
    "    chambre_chained = float(models[\"chambre_from_area_piece\"].predict(X_chain)[0])\n",
    "    chambre_chained = max(0.0, min(piece_hat, chambre_chained))\n",
    "    chambre_chained_round = int(round(min(piece_hat_round, chambre_chained)))\n",
    "\n",
    "    if chambre_direct_round == piece_hat_round :\n",
    "        chambre_direct_round -= 1\n",
    "        chambre_direct -= 1\n",
    "    if chambre_chained_round == piece_hat_round :\n",
    "        chambre_chained -= 1\n",
    "        chambre_chained_round -= 1 \n",
    "\n",
    "    return {\n",
    "        \"piece_hat\": piece_hat,\n",
    "        \"piece_hat_round\": piece_hat_round,\n",
    "        \"chambre_direct\": chambre_direct,\n",
    "        \"chambre_direct_round\": chambre_direct_round,\n",
    "        \"chambre_chained\": chambre_chained,\n",
    "        \"chambre_chained_round\": chambre_chained_round,\n",
    "    }\n",
    "\n",
    "# ---------- end-to-end: train + evaluate ----------\n",
    "def train_and_evaluate_option_b(\n",
    "    train_df: pd.DataFrame,\n",
    "    eval_df: pd.DataFrame | None = None,\n",
    "    alpha_piece: float = 1.0,\n",
    "    alpha_ch_direct: float = 1.0,\n",
    "    alpha_ch_chain: float = 1.0,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    # ensure numeric\n",
    "    def to_num(df):\n",
    "        out = df.copy()\n",
    "        for c in [\"AREA\", \"PIECE\", \"CHAMBRE\"]:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "        return out\n",
    "\n",
    "    if eval_df is None:\n",
    "        tr_df, te_df = train_test_split(to_num(train_df), test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        tr_df, te_df = to_num(train_df), to_num(eval_df)\n",
    "\n",
    "    models = train_option_b(tr_df, alpha_piece, alpha_ch_direct, alpha_ch_chain)\n",
    "    metrics = {}\n",
    "\n",
    "    # PIECE <- AREA\n",
    "    te_piece = te_df.dropna(subset=[\"PIECE\"])\n",
    "    if len(te_piece) > 0:\n",
    "        p_pred = models[\"piece_from_area\"].predict(te_piece[[\"AREA\"]])\n",
    "        p_pred = as_float(p_pred)\n",
    "\n",
    "        p_mae = mean_absolute_error(te_piece[\"PIECE\"], p_pred)\n",
    "        p_r2  = r2_score(te_piece[\"PIECE\"], p_pred)\n",
    "\n",
    "        p_pred_pp    = np.maximum(1.0, p_pred)\n",
    "        p_pred_round = round_int(p_pred_pp)\n",
    "\n",
    "        piece_true_int = te_piece[\"PIECE\"].astype(int).to_numpy()\n",
    "        p_exact  = float(np.mean(p_pred_round == piece_true_int))\n",
    "        p_within = float(np.mean(np.abs(p_pred_round - piece_true_int) <= 1))\n",
    "\n",
    "        metrics[\"PIECE_from_AREA\"] = {\n",
    "            \"MAE\": float(p_mae), \"R2\": float(p_r2),\n",
    "            \"pct_exact\": p_exact, \"pct_within_1\": p_within,\n",
    "        }\n",
    "\n",
    "    # CHAMBRE (direct) <- AREA\n",
    "    te_ch = te_df.dropna(subset=[\"CHAMBRE\"])\n",
    "    if len(te_ch) > 0:\n",
    "        c_pred_d = models[\"chambre_from_area\"].predict(te_ch[[\"AREA\"]])\n",
    "        c_pred_d = as_float(c_pred_d)\n",
    "\n",
    "        c_mae_d = mean_absolute_error(te_ch[\"CHAMBRE\"], c_pred_d)\n",
    "        c_r2_d  = r2_score(te_ch[\"CHAMBRE\"], c_pred_d)\n",
    "\n",
    "        piece_true = te_ch[\"PIECE\"].to_numpy(dtype=float) if \"PIECE\" in te_ch.columns else np.full(len(te_ch), np.inf)\n",
    "\n",
    "        c_pred_d_pp    = np.maximum(0.0, c_pred_d)\n",
    "        c_pred_d_clip  = np.minimum(piece_true, c_pred_d_pp)\n",
    "        c_pred_d_round = round_int(c_pred_d_clip)\n",
    "\n",
    "        ch_true_int = te_ch[\"CHAMBRE\"].astype(int).to_numpy()\n",
    "        c_exact_d  = float(np.mean(c_pred_d_round == ch_true_int))\n",
    "        c_within_d = float(np.mean(np.abs(c_pred_d_round - ch_true_int) <= 1))\n",
    "\n",
    "        metrics[\"CHAMBRE_from_AREA (direct)\"] = {\n",
    "            \"MAE\": float(c_mae_d), \"R2\": float(c_r2_d),\n",
    "            \"pct_exact\": c_exact_d, \"pct_within_1\": c_within_d,\n",
    "        }\n",
    "\n",
    "    # CHAMBRE (chained)\n",
    "    te_ch_chain_true = te_df.dropna(subset=[\"CHAMBRE\", \"PIECE\"])\n",
    "    if len(te_ch_chain_true) > 0:\n",
    "        # (a) True piece\n",
    "        c_pred_ct = models[\"chambre_from_area_piece\"].predict(te_ch_chain_true[[\"AREA\", \"PIECE\"]])\n",
    "        c_pred_ct = as_float(c_pred_ct)\n",
    "\n",
    "        c_mae_ct = mean_absolute_error(te_ch_chain_true[\"CHAMBRE\"], c_pred_ct)\n",
    "        c_r2_ct  = r2_score(te_ch_chain_true[\"CHAMBRE\"], c_pred_ct)\n",
    "\n",
    "        c_pred_ct_pp    = np.maximum(0.0, np.minimum(te_ch_chain_true[\"PIECE\"].to_numpy(dtype=float), c_pred_ct))\n",
    "        c_pred_ct_round = round_int(c_pred_ct_pp)\n",
    "\n",
    "        ch_true_int = te_ch_chain_true[\"CHAMBRE\"].astype(int).to_numpy()\n",
    "        c_exact_ct  = float(np.mean(c_pred_ct_round == ch_true_int))\n",
    "        c_within_ct = float(np.mean(np.abs(c_pred_ct_round - ch_true_int) <= 1))\n",
    "\n",
    "        metrics[\"CHAMBRE_from_(AREA,PIECE) (true piece)\"] = {\n",
    "            \"MAE\": float(c_mae_ct), \"R2\": float(c_r2_ct),\n",
    "            \"pct_exact\": c_exact_ct, \"pct_within_1\": c_within_ct,\n",
    "        }\n",
    "\n",
    "        # (b) Area-only path: pieceÌ‚ first, then chambreÌ‚\n",
    "        p_hat = models[\"piece_from_area\"].predict(te_ch_chain_true[[\"AREA\"]])\n",
    "        p_hat = as_float(p_hat)\n",
    "        p_hat_pp = np.maximum(1.0, p_hat)\n",
    "\n",
    "        Xc = pd.DataFrame({\"AREA\": te_ch_chain_true[\"AREA\"].values, \"PIECE\": p_hat_pp})\n",
    "        c_pred_ca = models[\"chambre_from_area_piece\"].predict(Xc)\n",
    "        c_pred_ca = as_float(c_pred_ca)\n",
    "\n",
    "        c_pred_ca_pp    = np.maximum(0.0, np.minimum(p_hat_pp, c_pred_ca))\n",
    "        c_pred_ca_round = round_int(c_pred_ca_pp)\n",
    "\n",
    "        c_mae_ca = mean_absolute_error(te_ch_chain_true[\"CHAMBRE\"], c_pred_ca)\n",
    "        c_r2_ca  = r2_score(te_ch_chain_true[\"CHAMBRE\"], c_pred_ca)\n",
    "\n",
    "        c_exact_ca  = float(np.mean(c_pred_ca_round == ch_true_int))\n",
    "        c_within_ca = float(np.mean(np.abs(c_pred_ca_round - ch_true_int) <= 1))\n",
    "\n",
    "        metrics[\"CHAMBRE_chained (AREAâ†’PIECEÌ‚â†’CHAMBRE)\"] = {\n",
    "            \"MAE\": float(c_mae_ca), \"R2\": float(c_r2_ca),\n",
    "            \"pct_exact\": c_exact_ca, \"pct_within_1\": c_within_ca,\n",
    "        }\n",
    "\n",
    "    return models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "33b4a5c9-2aca-4df7-be55-0d32a9a902c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_predict_features(df,models):\n",
    "    preds = {}\n",
    "    mask_both_missing = df[\"PIECE\"].isna() & df[\"CHAMBRE\"].isna()\n",
    "    mask_piece_missing = df[\"PIECE\"].isna() & df[\"CHAMBRE\"].notna()\n",
    "    mask_chambre_missing = df[\"CHAMBRE\"].isna() & df[\"PIECE\"].notna()\n",
    "\n",
    "    out = df.copy(deep=True)\n",
    "    if mask_chambre_missing.any():\n",
    "        X1 = out.loc[mask_chambre_missing,[\"AREA\",\"PIECE\"]].astype(float)\n",
    "        out.loc[mask_chambre_missing, \"CHAMBRE\"]=np.round(\n",
    "            np.maximum(0.0,np.minimum(\n",
    "                X1[\"PIECE\"].to_numpy(),\n",
    "                models[\"chambre_from_area_piece\"].predict(X1).astype(float)\n",
    "                )\n",
    "            )\n",
    "        ).astype(int)\n",
    "\n",
    "    if mask_both_missing.any():\n",
    "        preds_by_area = out.loc[mask_both_missing,\"AREA\"].apply(lambda x : \n",
    "                                                                predict_from_area(models=models,area_value=float(x))\n",
    "                                                               )\n",
    "        out.loc[mask_both_missing,\"PIECE\"] = preds_by_area.apply(lambda x:x[\"piece_hat_round\"] if isinstance(x,dict) else 0)\n",
    "        out.loc[mask_both_missing,\"CHAMBRE\"] = preds_by_area.apply(lambda x:x[\"chambre_chained_round\"] if isinstance(x,dict) else 0)\n",
    "\n",
    "    if mask_piece_missing.any():\n",
    "        out.loc[mask_piece_missing,\"PIECE\"] = out.loc[mask_piece_missing,\"CHAMBRE\"].astype(float) + 1         \n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "646e7ebe-413a-4e22-a6b6-57fa3f1a037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset_seloger(csv_file : str = None):\n",
    "    if not csv_file :\n",
    "        raise ValueError(\"Require dataset filepath\")\n",
    "    #vopen file\n",
    "    try :\n",
    "        raw_df = pd.read_csv(csv_file)\n",
    "    except Exception :\n",
    "        raise ValueError(\"Error converting file.csv to dataframe\")\n",
    "    raw_df_cols = raw_df.columns.tolist()\n",
    "    #verify columns\n",
    "    if raw_df_cols != ['href', 'title']:\n",
    "        raise ValueError(f\"Unexpected columns {raw_df_cols}\")\n",
    "\n",
    "    \n",
    "    #1 extract price,unit-price, data-tags -------------------------------\n",
    "    out_df = get_processed(raw_df[\"title\"]) \n",
    "    #2 clean tags (remove numbers, normalize them)\n",
    "    out_df = out_df.loc[out_df[\"DATA TAG\"].notna()]\n",
    "    out_df[\"CLEANED TAG\"] = out_df[\"DATA TAG\"].apply(\n",
    "        lambda x : normalize_tags(x) if isinstance(x,(tuple,list,set)) else pd.NA\n",
    "    )\n",
    "    out_df = out_df.loc[out_df[\"CLEANED TAG\"].notna()]\n",
    "    #3 extract type\n",
    "    out_df[\"TYPE\"] = out_df[\"CLEANED TAG\"].apply(\n",
    "        lambda x : extract_type(x) if isinstance(x,(tuple,list,set)) else pd.NA\n",
    "    )\n",
    "    out_df = out_df.loc[out_df[\"TYPE\"].notna()]\n",
    "\n",
    "    #4 extract location(postcode)\n",
    "    out_df = extract_loc(df=out_df,dict_map=STEM_TO_POSTCODES)\n",
    "    out_df = out_df.loc[out_df[\"LOC\"].notna()]\n",
    "    if out_df[\"LOC\"].apply(lambda x : len(x) == 1 if isinstance(x,(set)) else False).any():\n",
    "        out_df[\"LOC\"] = out_df[\"LOC\"].apply(lambda x : next(iter(x)))\n",
    "    else :\n",
    "        raise ValueError(\"Multiple location in one row detected.\")\n",
    "    #5 extract area,terrain\n",
    "    out_df = extract_area(out_df)\n",
    "    out_df = out_df.loc[~(out_df[\"AREA\"].isna() & out_df[\"TERRAIN\"].isna())]\n",
    "    out_df.loc[out_df[\"TERRAIN\"].isna(),\"TERRAIN\"] = 0\n",
    "    out_df.loc[out_df[\"AREA\"].isna(),\"AREA\"] = 0\n",
    "    out_df[\"AREA\"] = out_df[\"AREA\"].astype(float)\n",
    "    out_df[\"TERRAIN\"] = out_df[\"TERRAIN\"].astype(float)\n",
    "    #extract feature\n",
    "    out_df = extract_additionals(out_df)\n",
    "    out_df[\"FLOOR\"] = out_df[\"FLOOR\"].astype(int)\n",
    "    out_df[\"FLOORS\"] = out_df[\"FLOORS\"].astype(int)\n",
    "    #fill missing values for chambre and piece, use linear regression model\n",
    "    models,_ = train_and_evaluate_option_b(out_df.dropna())\n",
    "    out_df = lin_predict_features(df=out_df,models=models)\n",
    "    out_df[\"CHAMBRE\"] = out_df[\"CHAMBRE\"].astype(int)\n",
    "    out_df[\"PIECE\"] = out_df[\"PIECE\"].astype(int)\n",
    "    out_df.loc[out_df[\"TYPE\"] == \"TERRAIN CONSTRUCTIBL A VENDR\" ,[\"PIECE\",\"CHAMBRE\",\"FLOOR\",\"FLOORS\"]] =0\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "fcfd9e13-822d-4866-8999-efc84d64e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thaim\\AppData\\Local\\Temp\\ipykernel_126624\\3798004323.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mask_has_chambr = df[col_seq].apply(lambda x : has_chambr(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
      "C:\\Users\\thaim\\AppData\\Local\\Temp\\ipykernel_126624\\3798004323.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mask_has_piece = df[col_seq].apply(lambda x : has_piece(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
      "C:\\Users\\thaim\\AppData\\Local\\Temp\\ipykernel_126624\\3798004323.py:109: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['ETAG 5/6' 'ETAG 1/7' 'RDC/1' ... 'ETAG 2/10' 'ETAG 2/6' 'ETAG 5/9']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  series_floor_tag.loc[mask_floor_any] = df.loc[mask_has_floor, col_tag].apply(lambda x: get_floor_tag(x) if x else pd.NA)\n"
     ]
    }
   ],
   "source": [
    "vectorized_dataset = vectorize_dataset_seloger(\"data_sample_1584.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7e60c-db6a-495d-9315-d3e8a433686c",
   "metadata": {},
   "source": [
    "#### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "98fec72a-6775-4393-8959-29123d55b8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAW                  0\n",
       "DATA TAG             0\n",
       "PRICE                0\n",
       "PRICE_UNIT           0\n",
       "CLEANED TAG          0\n",
       "TYPE                 0\n",
       "LOC                  0\n",
       "ALL AREA             0\n",
       "AREA                 0\n",
       "TERRAIN              0\n",
       "DISPONIBL MAINTEN    0\n",
       "NEUF                 0\n",
       "CHAMBRE              0\n",
       "PIECE                0\n",
       "FLOOR                0\n",
       "FLOORS               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "b7ba3007-d24b-4359-b1e2-34b6bd67a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RAW', 'DATA TAG', 'PRICE', 'PRICE_UNIT', 'CLEANED TAG', 'TYPE', 'LOC',\n",
       "       'ALL AREA', 'AREA', 'TERRAIN', 'DISPONIBL MAINTEN', 'NEUF', 'CHAMBRE',\n",
       "       'PIECE', 'FLOOR', 'FLOORS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "2f7b7509-b516-4b96-844e-d3d8878ee43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DIVISIBL', 'HOTEL PARTICULI A VENDR', 'TERRAIN'}"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verification for valid tags, there's some tags that's included in the output vectorized_dataset\n",
    "Explanations : \n",
    "- DIVISIBLE was removed from filtering types process\n",
    "- HOTEL PARTICULI A VENDR was later decided not to be used since our models focus on TERRAIN CONSTRUCTIBLE, MAISON, APPART (residential properties)\n",
    "- TERRAIN is extracted to AREA,TERRAIN cols\n",
    "\"\"\"\n",
    "set(vectorized_dataset[\"CLEANED TAG\"].explode().tolist()).symmetric_difference(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "8c75e252-048d-4eb1-8bfc-e31385434f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APPART A VENDR',\n",
       " 'DUPLEX A VENDR',\n",
       " 'LOFT A VENDR',\n",
       " 'MAISON A VENDR',\n",
       " 'MAISON VILL A VENDR',\n",
       " 'STUDIO A VENDR',\n",
       " 'TERRAIN CONSTRUCTIBL A VENDR',\n",
       " 'VILL A VENDR'}"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify type\n",
    "set(vectorized_dataset[\"TYPE\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "02dafad8-cddf-4310-971e-f44e67f72524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIECE :  15\n",
      "RAW :  Maison Ã  vendre - Paris 17Ã¨me - 5â€¯100â€¯000Â â‚¬ - 15 piÃ¨ces, 6 chambres, 500,3 mÂ² \n",
      " ****************************************************************************************************\n",
      "FLOORS :  32\n",
      "32\n",
      "RAW :  Appartement Ã  vendre - Paris 13Ã¨me - 549â€¯000Â â‚¬ - 5 piÃ¨ces, 3 chambres, 96 mÂ², Ã‰tage 28/32 \n",
      " ****************************************************************************************************\n",
      "FLOOR :  30\n",
      "RAW :  Appartement Ã  vendre - Paris 15Ã¨me - 410â€¯000Â â‚¬ - 2 piÃ¨ces, 1 chambre, 48 mÂ², Ã‰tage 30/31 \n",
      " ****************************************************************************************************\n",
      "AREA :  500.3\n",
      "RAW :  Maison Ã  vendre - Paris 17Ã¨me - 5â€¯100â€¯000Â â‚¬ - 15 piÃ¨ces, 6 chambres, 500,3 mÂ² \n",
      " ****************************************************************************************************\n",
      "TERRAIN :  499.0\n",
      "RAW :  Maison de ville Ã  vendre - Montreuil - 750â€¯000Â â‚¬ - 8 piÃ¨ces, 6 chambres, 240 mÂ², 499 mÂ² de terrain \n",
      " ****************************************************************************************************\n",
      "CHAMBRE :  9\n",
      "RAW :  Maison Ã  vendre - Paris 16Ã¨me - 7â€¯400â€¯000Â â‚¬ - 12 piÃ¨ces, 9 chambres, 375 mÂ² \n",
      " ****************************************************************************************************\n",
      "PRICE :  10500000.0\n",
      "RAW :  Appartement Ã  vendre - Paris 16Ã¨me - 10â€¯500â€¯000Â â‚¬ - 8 piÃ¨ces, 4 chambres, 374 mÂ², Ã‰tage 5/5 \n",
      " ****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#verify cols values\n",
    "def get_max_val(df=vectorized_dataset,col=None):\n",
    "    max_v = vectorized_dataset.loc[vectorized_dataset[col] == max(vectorized_dataset[col])]\n",
    "    print(f\"{col} : \",max_v[col].to_string(index = False))\n",
    "    print(\"RAW : \",max_v[\"RAW\"].iloc[0],\"\\n\",\"*\"*100)\n",
    "check_cols = {\"AREA\",\"TERRAIN\",\"CHAMBRE\",\"PIECE\",\"FLOOR\",\"FLOORS\",\"PRICE\"}\n",
    "for c in check_cols :\n",
    "    get_max_val(col=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "758a0711-7074-46cd-8599-3bece22eeaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIECE :  0\n",
      "RAW :  Terrain constructible Ã  vendre - Montreuil - 450â€¯000Â â‚¬ - 394 mÂ² de terrain \n",
      " ****************************************************************************************************\n",
      "FLOORS :  0\n",
      "RAW :  Duplex Ã  vendre - Neuf - Saint-Cloud - 1â€¯140â€¯000Â â‚¬ - 4 piÃ¨ces, 3 chambres, 100,8 mÂ², RDC \n",
      " ****************************************************************************************************\n",
      "FLOOR :  0\n",
      "RAW :  Appartement Ã  vendre - Paris 18Ã¨me - 1â€¯550â€¯000Â â‚¬ - 5 piÃ¨ces, 4 chambres, 191,9 mÂ², RDC/1 \n",
      " ****************************************************************************************************\n",
      "AREA :  0.0\n",
      "RAW :  Terrain constructible Ã  vendre - Montreuil - 450â€¯000Â â‚¬ - 394 mÂ² de terrain \n",
      " ****************************************************************************************************\n",
      "TERRAIN :  0.0\n",
      "RAW :  Appartement Ã  vendre - Paris 1er - 560â€¯000Â â‚¬ - 2 piÃ¨ces, 1 chambre, 45 mÂ², Ã‰tage 5/6 \n",
      " ****************************************************************************************************\n",
      "CHAMBRE :  0\n",
      "RAW :  Studio Ã  vendre - Paris 12Ã¨me - 115â€¯000Â â‚¬ - 1 piÃ¨ce, 8,3 mÂ², Ã‰tage 8/8 \n",
      " ****************************************************************************************************\n",
      "PRICE :  70000.0\n",
      "RAW :  Appartement Ã  vendre - Paris 16Ã¨me - 70â€¯000Â â‚¬ - 1 piÃ¨ce, 9,1 mÂ², Ã‰tage 7/7 \n",
      " ****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def get_min_val(df=vectorized_dataset,col=None):\n",
    "    max_v = vectorized_dataset.loc[vectorized_dataset[col] == min(vectorized_dataset[col])].iloc[0]\n",
    "    print(f\"{col} : \",max_v[col])\n",
    "    print(\"RAW : \",max_v.iloc[0],\"\\n\",\"*\"*100)\n",
    "#check_cols = {\"AREA\",\"TERRAIN\",\"CHAMBRE\",\"PIECE\",\"FLOOR\",\"FLOORS\",\"PRICE\"}\n",
    "for c in check_cols :\n",
    "    get_min_val(col=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "4390de33-730e-43d3-9640-9fa07ee2f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do piece less than chambre ?\n",
    "len(vectorized_dataset.loc[vectorized_dataset[\"PIECE\"] < vectorized_dataset[\"CHAMBRE\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "ea61f31f-8e7e-49ce-8d04-62b21924ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seem good, let's save\n",
    "vectorized_dataset.to_csv(\"vectorized_dataset_seloger_1584.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980fc0f-bcbb-4545-9a22-c8936e820c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e5875-05df-4aaf-88a8-ae2401a79480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed188a32-94dc-4415-9df6-ebd695b584eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93c7f0-49bd-4523-a150-e0a76c411589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
