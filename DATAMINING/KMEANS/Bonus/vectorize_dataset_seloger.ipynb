{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b18587-8695-434c-a286-32dde24f9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manipulate/organize data/visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#preprocess data\n",
    "import unicodedata as ud\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#Linear regression to fill missing data for certain types\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c23f467-2173-472b-8920-74f0ed09a6f1",
   "metadata": {},
   "source": [
    "#### How to use ? :\n",
    "    you need a scrape data file from seloger which has at least one col \"title\" that contains data of each property scraped from \n",
    "    titles. use files - data_sample_{num_rows}.csv files , num_rows indicate size of the dataset. at the time of writing,\n",
    "    we have 2 files one contain ~1600 properties and one ~4743 (include both residential and commercial), we will select only residential in this code.\n",
    "\n",
    "    to use, you simple get the dataframe of the imported csv file or give the function filepath.\n",
    "    ex : \n",
    "        vectorize_dataset_seloger(df=merged_df) // this is for pandas.DataFrame input\n",
    "        vectorize_dataset_seloger(csv_file = \"folder/fname\") // this is for direct import\n",
    "    if you want to save the processed df :\n",
    "        vectorize_dataset_seloger(df=merged_df, save = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03416c4c-6b49-4019-b60b-9d13926d5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    the first dataset (about 1500 properties) i scraped only contain these locations. \n",
    "    (i carefully extracted all preprocessed tokens and collect all distinct locations to make this list,\n",
    "    i added some exception for some locations because my preprocessing code don't reduce them exactly like some other do.\n",
    "    Although it's not me who assigned them the postcodes.\n",
    "\"\"\"\n",
    "STEM_TO_POSTCODES = {\n",
    "    \"BOULOGN\": \"92000\",                       # Boulogne-Billancourt\n",
    "    \"BILLANCOURT\": \"92000\",                   # Boulogne-Billancourt\n",
    "    \"BOULOGN BILLANCOURT\": \"92000\",           # Boulogne-Billancourt\n",
    "    \"NEUILLY\": \"92000\",                       # Neuilly-sur-Seine\n",
    "    \"NEUILLY SEIN\": \"92000\",                  # Neuilly-sur-Seine\n",
    "    \"LEVALLOIS PERRET\": \"92000\",              # Levallois-Perret\n",
    "    \"CLICHY\": \"92000\",                        # Clichy\n",
    "    \"CLICHY GAREN\": \"92000\",                  # Clichy-la-Garenne (same CP)\n",
    "    \"SAINT CLOUD\": \"92000\",                   # Saint-Cloud\n",
    "    \"CLOUD\": \"92000\",                         # Saint-Cloud\n",
    "    \"PUTEAU\": \"92000\",                        # Puteaux\n",
    "    \"SURESN\": \"92000\",                        # Suresnes\n",
    "    \"ISSY\": \"92000\",                          # Issy-les-Moulineaux\n",
    "    \"ISSY MOULINEAU\": \"92000\",                # Issy-les-Moulineaux\n",
    "    \"MOULINEAU\": \"92000\",                     # Issy-les-Moulineaux\n",
    "    \"MONTROUG\": \"92000\",                      # Montrouge\n",
    "    \"LE LIL\": \"93000\",                        # Les Lilas\n",
    "    \"AUBERVILLI\": \"93000\",                    # Aubervilliers\n",
    "    \"SAINT OUEN SEIN\": \"93000\",               # Saint-Ouen-sur-Seine\n",
    "    \"OUEN\": \"93000\",                          # Saint-Ouen-sur-Seine\n",
    "    \"CHARENTON\": \"94000\",                     # Charenton-le-Pont\n",
    "    \"CHARENTON PONT\" : \"94000\",\n",
    "    \"VANV\": \"92000\",                          # Vanves\n",
    "    \"MONTREUIL\": \"93000\",                     # Montreuil\n",
    "    \"PANTIN\": \"93000\",                        # Pantin\n",
    "    \"PONT\": \"94000\",                          # (mapped by you to Charenton-le-Pont)\n",
    "    \"SEIN\": \"92000\",                          # (mapped by you to Neuilly-sur-Seine)\n",
    "    \"SAINT DEN\" : \"93000\",\n",
    "    \"DEN\": \"93000\",                           # Saint-Denis\n",
    "    \"PARIS\": \"75000\",\n",
    "    \"BAGNOLET\": \"93000\"\n",
    "}\n",
    "#decide which tokens are valid, other tokens were not carefully analyzed by humans therefore would likely to produce errors, and inaccuracy.\n",
    "valid_tokens = {'APPART A VENDR',\n",
    "     'AUBERVILLI',\n",
    "     'BAGNOLET',\n",
    "     'BOULOGN BILLANCOURT',\n",
    "     'CHAMBR',\n",
    "     'CHARENTON PONT',\n",
    "     'CLICHY',\n",
    "     'CLICHY GAREN',\n",
    "     'DISPONIBL MAINTEN',\n",
    "     'DIVISIBL',\n",
    "     'DUPLEX A VENDR',\n",
    "     'ETAG',\n",
    "     'HOTEL PARTICULI A VENDR',\n",
    "     'ISSY MOULINEAU',\n",
    "     'LE LIL',\n",
    "     'LEVALLOIS PERRET',\n",
    "     'LOFT A VENDR',\n",
    "     'MAISON A VENDR',\n",
    "     'MAISON VILL A VENDR',\n",
    "     'MONTREUIL',\n",
    "     'MONTROUG',\n",
    "     'M¬≤',\n",
    "     'NEUF',\n",
    "     'NEUILLY SEIN',\n",
    "     'PANTIN',\n",
    "     'PARIS',\n",
    "     'PIEC',\n",
    "     'PUTEAU',\n",
    "     'RDC',\n",
    "     'SAINT CLOUD',\n",
    "     'SAINT DEN',\n",
    "     'SAINT OUEN SEIN',\n",
    "     'STUDIO A VENDR',\n",
    "     'SURESN',\n",
    "     'TERRAIN',\n",
    "     'TERRAIN CONSTRUCTIBL A VENDR',\n",
    "     'VANV',\n",
    "     'VILL A VENDR'}\n",
    "\"\"\"\n",
    "     We decide to only analyze residential properties and remove all commercial properties,\n",
    "     later, we will drop all type (categorical data, include postcodes) that do not meet necessary threshold number of datapoint (property).\n",
    "\"\"\"\n",
    "valid_types = {\n",
    "    'APPART A VENDR',\n",
    "    'DUPLEX A VENDR',\n",
    "    'LOFT A VENDR',\n",
    "    'MAISON A VENDR',\n",
    "    'MAISON VILL A VENDR',\n",
    "    'STUDIO A VENDR',\n",
    "    'TERRAIN CONSTRUCTIBL A VENDR',\n",
    "    'VILL A VENDR'\n",
    "}\n",
    "\n",
    "#regrex patterns to identify and extract or to replace patterns in string.\n",
    "# Currency tokens (symbol or word forms)\n",
    "\n",
    "EUR_PATTERN = re.compile(\n",
    "    r\"(‚Ç¨|\\b(?:eur|euro(?:s)?)\\b)\", re.IGNORECASE\n",
    ")\n",
    "# ‚úÖ Matches: \"‚Ç¨\", \"EUR\", \"euro\", \"euros\", \"Prix: 200 eur\"\n",
    "# ‚ùå Doesn‚Äôt: \"eurostar\" (word boundary blocks it), \"EUROPE\" (not 'euro' exactly)\n",
    "# ‚ö†Ô∏è Note: Won‚Äôt catch \"‚Ç¨1,200\" as a whole‚Äîthis only finds the currency token, not the number.\n",
    "\n",
    "GBP_PATTERN = re.compile(\n",
    "    r\"(¬£|\\b(?:gbp|pound(?:s)?)\\b)\", re.IGNORECASE\n",
    ")\n",
    "# ‚úÖ Matches: \"¬£\", \"GBP\", \"pound\", \"pounds\", \"200 gbp\"\n",
    "# ‚ùå Doesn‚Äôt: \"pounding\", \"compound\" (word boundary prevents false positives)\n",
    "# ‚ö†Ô∏è \"$\" in \"CA$\" is not relevant here; that‚Äôs for USD_PATTERN.\n",
    "\n",
    "USD_PATTERN = re.compile(\n",
    "    r\"(\\$|\\b(?:usd|dollar(?:s)?)\\b)\", re.IGNORECASE\n",
    ")\n",
    "# ‚úÖ Matches: \"$\", \"USD\", \"dollar\", \"dollars\", \"2 usd\"\n",
    "# ‚ùå Doesn‚Äôt: \"sandollar\" (needs word boundary), other currency symbols like \"C$\", \"A$\"\n",
    "# ‚ö†Ô∏è \"$\" is ambiguous across countries; consider context if you need only US dollars.\n",
    "\n",
    "# General number with optional decimal (dot or comma)\n",
    "NUM_PATTERN = re.compile(r\"\\d+(?:[.,]\\d+)?\")\n",
    "# ‚úÖ Matches: \"5\", \"12\", \"83.5\", \"1,75\", \"0003\"\n",
    "# ‚ùå Doesn‚Äôt: signed numbers like \"-3.2\" (see PATTERN_NUM), fractions like \"1/7\" (you said slash handled separately)\n",
    "# ‚ö†Ô∏è Will also match date parts (\"12/10\") individually.\n",
    "\n",
    "# French ordinal like \"1ER\", \"12EM\"\n",
    "ORDINAL_PATTERN = re.compile(r\"\\b\\d+(?:ER|EM)\\b\", flags=re.IGNORECASE)\n",
    "# ‚úÖ Matches: \"1ER\", \"2er\", \"12EM\", \"3em\"\n",
    "# ‚ùå Doesn‚Äôt: \"1√àRE\", \"2√àME\" (accented forms), \"1st\", \"2nd\"\n",
    "# ‚ö†Ô∏è If you need accented French ordinals, expand to include √àRE/√àME variants.\n",
    "\n",
    "# Exact \"PARIS\" (uppercase only)\n",
    "PARIS_PATTERN = re.compile(r\"PARIS\")\n",
    "# ‚úÖ Matches: \"PARIS\"\n",
    "# ‚ùå Doesn‚Äôt: \"Paris\", \"paris\" (case-sensitive), arrondissements like \"75015 Paris\" unless uppercased\n",
    "# üí° Consider re.compile(r\"\\bparis\\b\", re.I) if you want case-insensitive.\n",
    "\n",
    "# Signed integer/float with dot decimal\n",
    "PATTERN_NUM = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n",
    "# ‚úÖ Matches: \"-12\", \"+3.5\", \"0.99\"\n",
    "# ‚ùå Doesn‚Äôt: comma decimals (\"1,75\"), fractions (\"1/7\")\n",
    "# ‚ö†Ô∏è This one is stricter than NUM_PATTERN (only dot decimals, allows sign).\n",
    "\n",
    "# Area cues: \"m¬≤\", \"m2\", \"surface‚Ä¶\", or \"terrain\"\n",
    "PATTERN_AREA = re.compile(r\"(?:[-‚Äì‚Äî]?\\s*)(?:M(?:¬≤|2)|SURFAC\\w*|TERRAIN)\", re.I)\n",
    "# ‚úÖ Matches: \"m¬≤\", \"M2\", \"surface\", \"surfaces\", \"terrain\", with optional preceding dash/space\n",
    "# ‚ùå Doesn‚Äôt: \"m^2\" (caret), \"area\" (English), \"superficie\" (if that term appears, it‚Äôs missed)\n",
    "# ‚ö†Ô∏è \"TERRAIN\" here might be too broad if you treat lots vs. living area differently.\n",
    "\n",
    "# Standalone \"TERRAIN\"\n",
    "PATTERN_TERRAIN = re.compile(r\"\\bTERRAIN\\b\", re.I)\n",
    "# ‚úÖ Matches only the word \"terrain\" as a token\n",
    "# ‚ùå Doesn‚Äôt: \"terrainment\", \"souterrain\" (good‚Äîno false positives)\n",
    "\n",
    "# Any digit anywhere\n",
    "PATTERN_HAS_DIGIT = re.compile(r\"\\d\")\n",
    "# ‚úÖ Matches if string contains at least one digit\n",
    "# ‚ùå Doesn‚Äôt: strings with numbers written as words (\"trois\", \"three\")\n",
    "\n",
    "# Availability \"disponible maintenant\" with flexible endings\n",
    "PATTERN_DISPO_NOW = re.compile(r\"\\bDISPONIBL\\w*\\s+MAINTEN\\w*\\b\", re.I)\n",
    "# ‚úÖ Matches: \"disponible maintenant\", \"Disponibilit√© maintenue\" (careful), \"disponibles maintien‚Ä¶\"\n",
    "# ‚ùå Doesn‚Äôt: rearranged word order (\"maintenant disponible\"), abbreviations\n",
    "# ‚ö†Ô∏è Broad \\w* can overmatch (\"maintenue\"); tighten if needed to specifically \"disponible maintenant\".\n",
    "\n",
    "# Floor indicators: \"√©tage\" or \"RDC\" (ground floor)\n",
    "PATTERN_FLOOR = re.compile(r\"\\b(?:ETAG\\w*|RDC)\\b\", re.I)\n",
    "# ‚úÖ Matches: \"etage\", \"√©tages\", \"RDC\"\n",
    "# ‚ùå Doesn‚Äôt: \"Rez-de-chauss√©e\" written fully (unless you rely on \"RDC\")\n",
    "# ‚ö†Ô∏è You also have dedicated ETAG/RDC patterns below; avoid double work.\n",
    "    \n",
    "PATTERN_ETAG = re.compile(r\"\\bETAG\\w*\\b\", re.I)\n",
    "# ‚úÖ Matches: \"etage\", \"√©tages\", \"√©tag√©\" (could be false positive)\n",
    "# ‚ùå Doesn‚Äôt: \"Rez-de-chauss√©e\", \"R+1\" (common shorthand not covered)\n",
    "\n",
    "PATTERN_RDC = re.compile(r\"\\bRDC\\w*\\b\", re.I)\n",
    "# ‚úÖ Matches: \"RDC\", \"RDC+1\" (because \\w*), \"RDC.\"\n",
    "# ‚ùå Doesn‚Äôt: \"Rez de chauss√©e\" written without \"RDC\"\n",
    "# ‚ö†Ô∏è If you don‚Äôt want \"RDC+1\", use r\"\\bRDC\\b\".\n",
    "\n",
    "# Rooms / pieces\n",
    "PATTERN_PIECE = re.compile(r\"\\bPIEC\\w*\\b\", re.I)\n",
    "# ‚úÖ Matches: \"piece\", \"pi√®ces\", \"pi√®ce(s)\"\n",
    "# ‚ùå Doesn‚Äôt: abbreviations like \"T2\", \"F3\" (common in FR real-estate)\n",
    "\n",
    "# New / newly built\n",
    "PATTERN_NEUF = re.compile(r\"\\bNEUF\\w*\\b\", re.I)\n",
    "# ‚úÖ Matches: \"neuf\", \"neuve\", \"neufs\", \"neuves\"\n",
    "# ‚ùå Doesn‚Äôt: \"neuf\" meaning the number 9 when used numerically (context needed)\n",
    "# ‚ö†Ô∏è Ambiguous: \"neuf\" can be adjective \"new\" or the noun/adjective \"nine\".\n",
    "\n",
    "# Bedrooms\n",
    "PATTERN_CHAMBRE = re.compile(r\"\\bCHAMBR\\w*\\b\", re.I)\n",
    "# ‚úÖ Matches: \"chambre\", \"chambres\"\n",
    "# ‚ùå Doesn‚Äôt: abbreviations (\"chb\", \"CH.\"), \"suite parentale\" (no keyword \"chambre\")\n",
    "\n",
    "# Number BEFORE a slash (capture group = the number)\n",
    "PATTERN_BEFORE_SLASH = re.compile(r\"([+-]?\\d+(?:\\.\\d+)?)\\s*/\")\n",
    "# ‚úÖ Matches the \"12\" in \"12 / 5\", \"+3.5/7\"\n",
    "# ‚ùå Doesn‚Äôt: \"12,5/7\" (comma decimals), no slash present\n",
    "# ‚ö†Ô∏è Will also match dates (\"12/10\")‚Äîdisambiguate upstream if needed.\n",
    "\n",
    "# Number AFTER a slash (capture group = the number)\n",
    "PATTERN_AFTER_SLASH = re.compile(r\"/\\s*([+-]?\\d+(?:\\.\\d+)?)\")\n",
    "# ‚úÖ Matches the \"5\" in \"12 / 5\", the \"7\" in \"3.5/7\"\n",
    "# ‚ùå Doesn‚Äôt: \"12/1,5\" (comma decimals)\n",
    "\n",
    "# Literal slash (useful for quick checks/splits)\n",
    "PATTERN_SLASH = re.compile(r\"/\")\n",
    "# ‚úÖ Matches any \"/\" character\n",
    "# ‚ùå Doesn‚Äôt: division written as \"√∑\" or \"per\" words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2d090a-3578-4207-bf17-b6c29f3ec55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text: str)->str:\n",
    "    \"\"\"\n",
    "    this function serve to remove accents and diacritics of french alphabet.\n",
    "    input string\n",
    "    output string\n",
    "    \"\"\"\n",
    "    #replace diacritics\n",
    "    text = (text.replace(\"≈ì\", \"oe\").replace(\"≈í\", \"OE\")\n",
    "           .replace(\"√¶\", \"ae\").replace(\"√Ü\", \"AE\"))\n",
    "    #decompose form '√®' -> 'e' + '`'\n",
    "    text = ud.normalize(\"NFD\",text)\n",
    "    #remove accents like '`','^',...\n",
    "    text = \"\".join([char for char in text if ud.category(char) != \"Mn\"])\n",
    "    #recompose \n",
    "    text = ud.normalize(\"NFC\",text)\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08354033-c66e-45c9-b63f-3dbd04b6f917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST GO GO'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_french_nltk(text: str)->str:\n",
    "    \"\"\"\n",
    "    this function serve to normalize tokens, preprocess the texts to better detect patterns,repetition of tokens.\n",
    "    input string\n",
    "    output string\n",
    "    \"\"\"\n",
    "    #get text that doesn't have accents.\n",
    "    text = remove_accents(text)\n",
    "    stemmer = SnowballStemmer(\"french\")#prepare stemming process for french.\n",
    "    stop_fr = set(stopwords.words(\"french\"))#prepare stopwords removal for french (stopwords are √† de la le etc..)\n",
    "    #extract tokens from text under french alphabet condition and m2/m^2 for extracting area value\n",
    "    tkns = re.findall(r\"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø\\d/(m2|m¬≤).]+\", text)\n",
    "    tkns = [t for t in tkns if t not in stop_fr]#adding to a list if token not in stopwords (to remove stopwords we defined earlier)\n",
    "    stems  = [stemmer.stem(t) for t in tkns]#stem, normalize words, for ex in english GO and GOES all normalized to GO (i dnt have ex in fr)\n",
    "    return (\" \".join(stems)).upper()\n",
    "preprocess_french_nltk(\"test : go goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0940405-90df-41c4-9b42-f4a4e95e009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- after brief visual inspection of the scraped data, we can find some patterns like, \n",
    "pieces of informations are separated by \"-\" and \",\".\n",
    "- but we have some \",\" that belongs to decimal numbers (e.g. 1,2 5,9 3,14 etc).\n",
    "- let's find the distinction between between seperation by \",\" and \",\" for decimals,\n",
    "we concluded that \",\" for decimal stick with numbers and \",\" to separate pieces of information is between 2 spaces.\n",
    "- let's replace \",\" for decimal by a \".\".\n",
    "\"\"\"\n",
    "def extract_tokens(text : str = \"\"):\n",
    "    #convert \",\" of float num to \".\"\n",
    "    text = re.sub(r\"(?<=\\d),(?=\\d)\",\".\",text)\n",
    "    #convert \"-\" to \" \"\n",
    "    text = re.sub(r'(?<=[^\\W\\d_])-(?=[^\\W\\d_])', ' ', text, flags=re.UNICODE)\n",
    "    tkns = [tkn.strip() for tkn in re.split(\"[-,]\",text)]\n",
    "    return tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc7e60b-723a-4d86-90e7-69f05861c362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528.0\n",
      "1564866.0\n"
     ]
    }
   ],
   "source": [
    "def convert_2price(price_str : str = None):\n",
    "    \"\"\"\n",
    "        to extract price,\n",
    "        price is in format like :\n",
    "        1234(some unicodedata)5678(some unicodedata)90\n",
    "        - we are just gonna get the digit and ignore the *special char with escape sequence*, \n",
    "        because big price don't need decimal numbers, it's useless (probably).\n",
    "    \"\"\"\n",
    "    try :\n",
    "        #get the point pattern (remember we convert all \",\" to \".\" if it's a decimal point)\n",
    "        where_is_the_point = re.compile(r\"\\.\")\n",
    "        #get matches\n",
    "        matches = list(where_is_the_point.finditer(price_str))\n",
    "        #initalize stop index (default'd be end of the input string)\n",
    "        stop_index = len(price_str)\n",
    "        #if theres a match, reassign stop index to the start index of the \".\"\n",
    "        if matches :\n",
    "            for match in matches :\n",
    "                stop_index = match.start()\n",
    "                break\n",
    "        price_str = \"\".join([chr for chr in price_str[:stop_index] if chr.isdigit()])#search only from 0 to the stop index\n",
    "        return float(price_str)\n",
    "    except Exception :\n",
    "        return None\n",
    "#testing.\n",
    "print(convert_2price(\"1528.15\"))\n",
    "print(convert_2price(r\"156‚ÄØ4866\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502f613f-c070-45c5-adf6-270437fd9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(tkns : list = None):\n",
    "    \"\"\"\n",
    "    -function to extract price token.\n",
    "    -input tokens : list (but actually it works both for tuple,set too but i only allow list here.)\n",
    "    -output list of [[*tokens with out price token*],*price token*]\n",
    "    \"\"\"\n",
    "    if not tkns :\n",
    "        raise ValueError(\"elements = None\")\n",
    "    #create patterns.\n",
    "    EUR_PATTERN = re.compile(r\"(‚Ç¨|\\b(?:eur|euro(?:s)?)\\b)\", re.IGNORECASE)\n",
    "    GBP_PATTERN = re.compile(r\"(¬£|\\b(?:gbp|pound(?:s)?)\\b)\", re.IGNORECASE)\n",
    "    USD_PATTERN = re.compile(r\"(\\$|\\b(?:usd|dollar(?:s)?)\\b)\", re.IGNORECASE)\n",
    "    #m_tkns for the tokens without price token, prices are price we find.\n",
    "    m_tkns = []\n",
    "    prices = []\n",
    "    #loop throught tokens to categorize.\n",
    "    for tkn in tkns :\n",
    "        if bool(EUR_PATTERN.search(tkn)):\n",
    "            prices.append([tkn,\"EUR\"])\n",
    "        elif bool(GBP_PATTERN.search(tkn)):\n",
    "            prices.append([tkn,\"GBP\"])\n",
    "        elif bool(USD_PATTERN.search(tkn)):\n",
    "            prices.append([tkn,\"USD\"])\n",
    "        else :\n",
    "            m_tkns.append(tkn)\n",
    "    if len(prices) != 1:\n",
    "        raise ValueError(\"More than 1 or no price for this property\")\n",
    "    return [m_tkns,prices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87e07d0-6e05-437d-a556-8095a3b24149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed(raw_df):\n",
    "    \"\"\"\n",
    "    just a function to apply all the functions i created above.\n",
    "    input df (pd.DataFrame or pandas.DataFrame depend how you assign alias)\n",
    "    output df with cols (names i mention in return line)\n",
    "    \"\"\"\n",
    "    list_tags = []\n",
    "    prices =[]\n",
    "    price_units = []\n",
    "    raws = []\n",
    "    for i in range(len(raw_df)):\n",
    "        tags,(price_str,price_unit)=extract_price(extract_tokens(raw_df.iloc[i]))\n",
    "        tags = list(map(preprocess_french_nltk,tags))\n",
    "        price = convert_2price(price_str)\n",
    "        if price == None :\n",
    "            print(price_str)\n",
    "        if price > 1000 :# add threshold to prevent false pricing, or maybe i just don't like extreme little houses/appartments\n",
    "            list_tags.append(tags)\n",
    "            prices.append(price)\n",
    "            price_units.append(price_unit)\n",
    "            raws.append(raw_df.iloc[i])\n",
    "    return (pd.DataFrame({\"RAW\" : tuple(raws),\"DATA TAG\":list_tags,\"PRICE\":prices,\"PRICE_UNIT\":price_units}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "944059f4-2ed6-4568-8a1e-35f5639e37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_token(tag):\n",
    "    \"\"\"\n",
    "    function to normalize tokens, well, in short, i just don't like meaningless variations like \"12eme PARIS\" or \"15eme PARIS\",\n",
    "    they are all \"PARIS\", so lets consider them this way, and don't worry, im not erasing the info of it's district,\n",
    "    just putting all these normalized tokens in a different column so it would be easier to find and to list and to analyze and to inspect.\n",
    "    but i suppose i can achieve the same result with regrex, but i find it much much simpler this way.\n",
    "    - input string\n",
    "    - output string, or None-(isn't an efficient way but we learn along the way)\n",
    "    \"\"\"\n",
    "    tag = tag.upper().strip()\n",
    "    if \"ETAG\" in tag :\n",
    "        return \"ETAG\"\n",
    "    if 'RDC' in tag:    \n",
    "        return 'RDC'\n",
    "    \"\"\"\n",
    "    if 'TERRAIN' in tag:\n",
    "        return 'TERRAIN'\n",
    "    \"\"\"\n",
    "    if 'SURFAC' in tag:\n",
    "        return 'SURFAC'\n",
    "    #remove tags\n",
    "    if 'DIVISIBL A PART M¬≤' in tag or 'DIVISIBL JUSQU A M¬≤' in tag or 'DIVISIBL M¬≤ A M¬≤' in tag:\n",
    "        return None\n",
    "    #reduce all M¬≤ to M¬≤\n",
    "    if 'M¬≤' in tag :\n",
    "        return 'M¬≤'\n",
    "\n",
    "    # Remove ordinals like 1ER / 12EM and plain numbers/decimals\n",
    "    tag = ORDINAL_PATTERN.sub('', tag)\n",
    "    tag = tag.replace('/', ' ')            # split fractions like \"1/7\"\n",
    "    tag = NUM_PATTERN.sub('', tag)\n",
    "\n",
    "    # Collapse whitespace\n",
    "    tag = re.sub(r'\\s+', ' ', tag).strip()\n",
    "\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa418fe8-5ad3-4df4-b81d-b8ae411dec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the normalize to tags (PLURAL)\n",
    "def normalize_tags(tags):\n",
    "    n_tkns = [] #normalized tokens\n",
    "    for tkn in tags :\n",
    "        if str(tkn).strip() :\n",
    "            n_tkn = normalize_token(tkn)\n",
    "            if n_tkn:\n",
    "                if n_tkn in valid_tokens :#verify if it's valid tokens.\n",
    "                    n_tkns.append(n_tkn)\n",
    "                else :\n",
    "                    return pd.NA\n",
    "    if len(n_tkns) == 0 :\n",
    "        return None\n",
    "    return tuple(n_tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c39f5923-6439-4391-ad0e-bf5b9c4b1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(seqs):\n",
    "    return {normalize_tags(tags) for tags in seqs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56786a4d-4f01-41c5-8981-a114c833220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_type(cleaned_tags):\n",
    "    \"\"\"\n",
    "    to extract type of property (\"APPART A VENDR\" or \"MAISON A VENDR\" , etc)\n",
    "    \"\"\"\n",
    "    for t in cleaned_tags :\n",
    "        if t in valid_types : # only return allowed types\n",
    "            return t\n",
    "    return pd.NA#to drop them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85881c09-e53f-4f67-be11-4ffefbbee8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paris_cp(tags):\n",
    "    \"\"\"\n",
    "    get paris codepostal.\n",
    "    it's designed for paris because, other districts don't matter. and we won't have that much data to futher divide categories.\n",
    "    - possible input list,tuple,set of tokens, also sometime tags can be referred to sequences (seqs).\n",
    "    - output : string (e.g. \"75015\",...)\n",
    "    \"\"\"\n",
    "    paris_tag = \"\"\n",
    "    for tag in tags :\n",
    "        if \"PARIS\" in tag :\n",
    "            if not bool(re.search(r\"\\d\", tag))  :#if find no digit\n",
    "                print(\"Got tag with no digit : \", tag)\n",
    "                return None\n",
    "            try :\n",
    "                digits = int(\"\".join([ch for ch in tag if ch.isdigit()]))\n",
    "                if digits <= 20 and digits > 0:\n",
    "                    return str(75000 + int(digits))\n",
    "                else :\n",
    "                    print(\"District code doesn't exist\")\n",
    "                    return None\n",
    "            except Exception :\n",
    "                print(f\"Error encountered (got digits = {digits}): \", tag)\n",
    "                return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "997807a3-8f8e-46bd-961d-dc88528598f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's extract all locations to a column\n",
    "def extract_loc(df,dict_map):\n",
    "    \"\"\"\n",
    "    extract_loc is gonna focus on inside paris, and others will be more generalized.\n",
    "    - inputs :\n",
    "        + df -> pd.DataFrame\n",
    "        + dict_map : a dict that contains locations mapped with codepostals.\n",
    "    - output : dataframe with loc col (extracted codepostals in string type)\n",
    "    \"\"\"\n",
    "    df= df.copy()\n",
    "    mask_not_paris = df[\"CLEANED TAG\"].apply(lambda x: isinstance(x, (list, tuple, set)) and \"PARIS\" not in x)\n",
    "    valid_locs = set(dict_map.keys())\n",
    "    df.loc[mask_not_paris, \"LOC\"] = (\n",
    "        df.loc[mask_not_paris, \"CLEANED TAG\"]\n",
    "          .apply(lambda x: (set(x) & valid_locs) if x else pd.NA)\n",
    "          .apply(lambda x: {dict_map[c] for c in x} if x else pd.NA)\n",
    "    )\n",
    "    df.loc[~mask_not_paris,\"LOC\"] = (\n",
    "        df[~mask_not_paris][\"DATA TAG\"].apply(lambda x : set([get_paris_cp(x)]) if get_paris_cp(x) else pd.NA)\n",
    "    )\n",
    "    mask_multi_loc = df[\"LOC\"].apply(lambda x: isinstance(x, (list, tuple, set)) and len(x) > 1)\n",
    "    #print(\"Multi loc assigned detected : \",(mask_multi_loc & mask_not_paris).sum())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fbb776d-5a73-45b8-8185-b074dea9a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numbers_in_text(s):\n",
    "    \"\"\"Return all numbers (as floats) found in a string. Spaces already preprocessed upstream.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s_clean = s.replace(\" \", \"\")  # e.g. '2 645' -> '2645'\n",
    "    return [float(m.group(0)) for m in PATTERN_NUM.finditer(s_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57a37631-3171-4e8d-afba-a58b1171203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's extract area, we know that area can be represented in 3 tags/tokens : \n",
    "-M¬≤\n",
    "-SURFAC\n",
    "-TERRAIN\n",
    "\"\"\"\n",
    "def extract_area(df):\n",
    "    \"\"\"\n",
    "    Normalize surface data into two scalars per row:\n",
    "      - AREA: one scalar for all non-TERRAIN surfaces (or TERRAIN tokens without digits)\n",
    "      - TERRAIN: one scalar for terrain (only if the token contains 'TERRAIN' + a digit)\n",
    "      - some special case will be handled (if there are), case where a token/tag contains \"DIVISIBL\", would often contains more than 1 area value,\n",
    "      we decide to take the min area for the indicated price (that's often how marketing do)\n",
    "    Inputs:\n",
    "      df: DataFrame\n",
    "    Returns:\n",
    "      new df with cols AREA,TERRAIN, both in float type.\n",
    "    \"\"\"\n",
    "    #create a set for tokens/tags that we area interested in extracting.\n",
    "    area_tags = {\"M¬≤\",\"SURFAC\",\"TERRAIN\"}\n",
    "    #create a mask for token/tag that contains patterns in area_tags.\n",
    "    has_area = df[\"CLEANED TAG\"].apply(\n",
    "        lambda xs: bool(set(xs) & set(area_tags)) if isinstance(xs, (list, tuple, set)) else False\n",
    "    )\n",
    "    #remove all rows that don't contain any area_tags, those miss the most crucial feature in determining a property value.\n",
    "    out = df.loc[has_area].copy()\n",
    "    #function to extract all tokens that match tags M,M^2,SURFAC,TERRAIN patterns.\n",
    "    def get_area_tokens(tokens):\n",
    "        \"\"\"\n",
    "        input : tokens type list,tuple,set (we are extracting from DATA TAG so it's likely to be a list.)\n",
    "        output : pd.NA if any or list of matched tokens.\n",
    "        \"\"\"\n",
    "        #just safety condition if tokens is type list,tuple,set\n",
    "        if not isinstance(tokens, (list, tuple, set)):\n",
    "            return pd.NA#assign them pd.NA\n",
    "        #a list to store matched tokens\n",
    "        hits = []\n",
    "        for t in tokens:#loop token in tokens\n",
    "            if re.search(PATTERN_AREA, str(t)):#if found\n",
    "                hits.append(str(t))#convert the token to string and add to hits list\n",
    "        if hits:\n",
    "            return hits\n",
    "        return pd.NA\n",
    "    #put all extracted tokens in a col named \"ALL AREA\" represent both AREA and TERRAIN scalars, we will separate them later.\n",
    "    out[\"ALL AREA\"] = out[\"DATA TAG\"].apply(\n",
    "        lambda x: get_area_tokens(list(x)) if isinstance(x, (list, tuple, set)) else pd.NA\n",
    "    )\n",
    "    #function to get the lowest number found in a tag/token (designed for tag that contains multiple area values like \"DIVISIBL\")\n",
    "    def get_lowest_value(tokens):\n",
    "        if not tokens:\n",
    "            return pd.NA\n",
    "        vals = []\n",
    "        for t in tokens:\n",
    "            try:\n",
    "                vals.extend(_numbers_in_text(t))#store \n",
    "            except Exception:\n",
    "                pass#if don't find any number or incompatible token/function\n",
    "        return min(vals) if vals else pd.NA\n",
    "    def is_divisible(tokens):\n",
    "        if not tokens:\n",
    "            return pd.NA\n",
    "        for t in tokens:\n",
    "            if isinstance(t, str) and \"DIVISIBL\" in t:\n",
    "                return True\n",
    "        return False\n",
    "    def split_area_vs_terrain(tokens):\n",
    "        if not isinstance(tokens, (list, tuple, set)):\n",
    "            return [], []\n",
    "        area_tokens, terrain_tokens = [], []\n",
    "        for t in tokens:\n",
    "            t_str = str(t)\n",
    "            has_terrain = bool(PATTERN_TERRAIN.search(t_str))#if tag/token has \"TERRAIN\" pattern.\n",
    "            has_digit   = bool(PATTERN_HAS_DIGIT.search(t_str))#same for digit.\n",
    "            if has_terrain and has_digit:\n",
    "                terrain_tokens.append(t_str)#that's a scalar for TERRAIN.\n",
    "            else:\n",
    "                area_tokens.append(t_str)#that's a scalar for AREA.\n",
    "        return area_tokens, terrain_tokens\n",
    "    #compute scalars for AREA and TERRAIN from 'ALL AREA'\n",
    "    def compute_scalars(tokens):\n",
    "        if not isinstance(tokens, (list, tuple, set)) or not tokens:\n",
    "            return pd.Series({\"AREA\": pd.NA, \"TERRAIN\": pd.NA})#in case theres nothing, we return with cols that contains pd.NA\n",
    "        #get area,terrain tokens.\n",
    "        area_tokens, terrain_tokens = split_area_vs_terrain(tokens)\n",
    "        if area_tokens:#area tokens is considered all tokens without \"TERRAIN\" pattern.\n",
    "            area_val = get_lowest_value(area_tokens)#in case have multiple area values in on area token (\"DIVISIBL\" case.)\n",
    "        else:\n",
    "            area_val = pd.NA\n",
    "        terrain_val = get_lowest_value(terrain_tokens) if terrain_tokens else pd.NA\n",
    "        return pd.Series({\"AREA\": area_val, \"TERRAIN\": terrain_val})\n",
    "    scalars = out[\"ALL AREA\"].apply(compute_scalars)\n",
    "    out.loc[:, \"AREA\"] = scalars[\"AREA\"]\n",
    "    out.loc[:, \"TERRAIN\"] = scalars[\"TERRAIN\"]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68113445-d735-4d7e-b7fc-c250ad8d99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool tag detect functions\n",
    "\"\"\"\n",
    "these are function that's to detect if a sequences (CLEANED TAG) col or (DATA TAG) contain a certain token.\n",
    "return true or false in digit (0,1).\n",
    "\"\"\"\n",
    "def is_dispo(tags):\n",
    "    # DISPONIBL MAINTEN: 1 if present, 0 if explicitly absent, <NA> if no tokens\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_DISPO_NOW.search(str(t)) for t in tags) else 0\n",
    "\n",
    "def is_neuf(tags):\n",
    "    # NEUF: 1 if present, 0 if explicitly absent, <NA> if no tokens\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_NEUF.search(str(t)) for t in tags) else 0\n",
    "\n",
    "def has_chambr(tags):\n",
    "    # CHAMBR: 1 if present, <NA> otherwise (to be averaged later, per your design)\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_CHAMBRE.search(str(t)) for t in tags) else pd.NA\n",
    "\n",
    "def has_piece(tags):\n",
    "    # PIEC: 1 if present, <NA> otherwise (to be averaged later)\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_PIECE.search(str(t)) for t in tags) else pd.NA\n",
    "\n",
    "def has_floor(tags):\n",
    "    # ETAG/RDC: 1 if present, 0 if explicitly absent, <NA> if no tokens\n",
    "    if not isinstance(tags, (set, tuple, list)) or len(tags) == 0:\n",
    "        return pd.NA\n",
    "    return 1 if any(PATTERN_FLOOR.search(str(t)) for t in tags) else 0\n",
    "\n",
    "#bool for floor extraction\n",
    "def is_rdc(tag):\n",
    "    if not tag :\n",
    "        raise ValueError(\"No tag provided\")\n",
    "    if bool(PATTERN_RDC.search(tag)):\n",
    "        return 1\n",
    "    else : return 0\n",
    "def is_etag(tag):\n",
    "    if not tag :\n",
    "        raise ValueError(\"No tag provided\")\n",
    "    if bool(PATTERN_ETAG.search(tag)):\n",
    "        return 1\n",
    "    else : return 0    \n",
    "def has_slash(tag):\n",
    "    if not tag:\n",
    "        raise ValueError(\"No tag provided\")\n",
    "    if bool(PATTERN_SLASH.search(tag)):\n",
    "        return 1\n",
    "    else : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd0f92a9-3613-44e9-9627-6bbbd94e4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_additionals(df,col_seq=\"CLEANED TAG\",col_tag=\"DATA TAG\"):\n",
    "    \"\"\"\n",
    "    extract all additional features: number of piece, number of chamber, number of floors, and which floor located,\n",
    "    is it new ? is it dispo mtn ?\n",
    "    - for floor feature (contains \"ETAG\" or \"RDC\" patterns) :\n",
    "        + have couple type of format like RDC/(digit) or (digit)/(digit) or RDC alone,\n",
    "        + here is how we treat them:\n",
    "            + RDC/(digit) -> floor = 0 and floors = (digit)\n",
    "            + RDC alone -> floor = 0 and floors = 0\n",
    "            + (digit)/(digit) -> floor = first digit before the slash and floors = second digit after the slash\n",
    "\n",
    "    and as usual, input a dataframe (this time there are parameters to indicate which cols to look for)\n",
    "    and output new dataframe with new cols for extracted features.\n",
    "    \"\"\"\n",
    "    #create masks\n",
    "    mask_is_dispo = df[col_seq].apply(lambda x : is_dispo(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_is_neuf = df[col_seq].apply(lambda x : is_neuf(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_has_chambr = df[col_seq].apply(lambda x : has_chambr(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_has_piece = df[col_seq].apply(lambda x : has_piece(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    mask_has_floor = df[col_seq].apply(lambda x : has_floor(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
    "    #extract tags\n",
    "    def get_chambr_tag(tags):\n",
    "        if not tags :\n",
    "            raise ValueError(\"Tags not provided\")\n",
    "        if not isinstance(tags,(set,list,tuple)):\n",
    "            return pd.NA\n",
    "        for t in tags :\n",
    "            if bool(PATTERN_CHAMBRE.search(t)):\n",
    "                return t\n",
    "        return pd.NA\n",
    "    def get_piece_tag(tags):\n",
    "        if not tags :\n",
    "            raise ValueError(\"Tags not provided\")\n",
    "        if not isinstance(tags,(set,list,tuple)):\n",
    "            return pd.NA\n",
    "        for t in tags :\n",
    "            if bool(PATTERN_PIECE.search(t)):\n",
    "                return t\n",
    "        return pd.NA\n",
    "    def get_floor_tag(tags):\n",
    "        if not tags :\n",
    "            raise ValueError(\"Tags not provided\")\n",
    "        if not isinstance(tags,(set,list,tuple)):\n",
    "            return pd.NA\n",
    "        for t in tags :\n",
    "            if bool(PATTERN_FLOOR.search(t)):\n",
    "                return t\n",
    "        return pd.NA\n",
    "    #get numerical values\n",
    "    def get_floor(tag):\n",
    "        if not tag :\n",
    "            raise ValueError(\"No tag provided\")\n",
    "        nb_floor = PATTERN_BEFORE_SLASH.search(tag)\n",
    "        if bool(nb_floor):\n",
    "            return int(float(nb_floor.group(1)))\n",
    "        else : return 0\n",
    "    def get_floors(tag):\n",
    "        if not tag :\n",
    "            raise ValueError(\"No tag provided\")\n",
    "        nb_floors = PATTERN_AFTER_SLASH.search(tag)\n",
    "        if bool(nb_floors):\n",
    "            return int(float(nb_floors.group(1)))\n",
    "        else : return 0\n",
    "    def get_num(tag):\n",
    "        tag = str(tag)\n",
    "        if not tag :\n",
    "            raise ValueError(\"tag not provided\")\n",
    "        num = PATTERN_NUM.search(tag)\n",
    "        if bool(num):\n",
    "            return int(num.group(0))\n",
    "        else :\n",
    "            return 0\n",
    "    #this function to split something like 5/7 to floor = 5 and floors = 7\n",
    "    def split_floor_tag(tag):\n",
    "        if not tag or not bool(PATTERN_FLOOR.search(tag)):#verify (safety check) if tag is provided and there must be floor patterns in tag\n",
    "            #floor patterns is ETAG or RDC\n",
    "            raise ValueError(\"tag not provided or no floor pat in tag\")\n",
    "        #we need to distinguish if taf is rdc or etag, cuz rdc don't have same format as etag\n",
    "        is_rdc_flag = bool(PATTERN_RDC.search(tag))\n",
    "        has_slash   = bool(PATTERN_SLASH.search(tag))\n",
    "        #default is floor = 0 and floors = 0\n",
    "        floor, floors = 0, 0\n",
    "        if is_rdc_flag:\n",
    "            floor = 0 #as mentioned.\n",
    "        if has_slash: #get digit before the slash and after the slash\n",
    "            nb_floor  = PATTERN_BEFORE_SLASH.search(tag)\n",
    "            nb_floors = PATTERN_AFTER_SLASH.search(tag)\n",
    "            if not is_rdc_flag and nb_floor: #case of RDC/(digit) \"and nb_floor\" mean if the code find number\n",
    "                floor = int(float(nb_floor.group(1)))\n",
    "            if nb_floors:\n",
    "                floors = int(float(nb_floors.group(1)))\n",
    "            if not nb_floors:\n",
    "                floors = floor\n",
    "        else:\n",
    "            if not is_rdc_flag:\n",
    "                floor = int(get_num(tag)) or 0\n",
    "            floors = floor\n",
    "    \n",
    "        return (floor, floors)\n",
    "    \"\"\"\n",
    "    these codes are to identify and extract numerical data.\n",
    "    \"\"\"\n",
    "    #bool extraction\n",
    "    series_dispo = df[col_seq].apply(lambda x : is_dispo(x))\n",
    "    series_neuf = df[col_seq].apply(lambda x : is_neuf(x))\n",
    "    #unconditional extraction & pd.NA to be averaged\n",
    "    #chambre\n",
    "    series_chambr = df[col_seq].apply(lambda x : has_chambr(x))\n",
    "    series_chambr.loc[series_chambr.notna()] = df.loc[mask_has_chambr,col_tag].apply(lambda x : get_chambr_tag(x))\n",
    "    series_chambr.loc[series_chambr.notna()] = series_chambr.loc[series_chambr.notna()].apply(lambda x : int(get_num(x)) if x else pd.NA)\n",
    "    is_valid_series_chambr = series_chambr.dropna().apply(lambda x : isinstance(x,int)).all()\n",
    "    if not is_valid_series_chambr :\n",
    "        raise ValueError(\"is_valid_series_chambr not valid\")\n",
    "    #piece\n",
    "    series_piece = df[col_seq].apply(lambda x : has_piece(x))\n",
    "    series_piece.loc[series_piece.notna()] = df.loc[mask_has_piece,col_tag].apply(lambda x : get_piece_tag(x))    \n",
    "    series_piece.loc[series_piece.notna()] = series_piece.loc[series_piece.notna()].apply(lambda x : int(get_num(x)) if x else pd.NA)\n",
    "    is_valid_series_piece = series_piece.dropna().apply(lambda x : isinstance(x,int)).all()\n",
    "    if not is_valid_series_piece :\n",
    "        raise ValueError(\"is_valid_series_piece not valid\")\n",
    "    #conditional extraction\n",
    "    series_floor_tag = df[col_seq].apply(lambda x: has_floor(x))\n",
    "    mask_floor_any = series_floor_tag.fillna(0).astype(bool)\n",
    "    series_floor_tag.loc[mask_floor_any] = df.loc[mask_has_floor, col_tag].apply(lambda x: get_floor_tag(x) if x else pd.NA)\n",
    "    series_floor_tag.loc[mask_floor_any] = series_floor_tag.loc[mask_floor_any].apply(lambda x: split_floor_tag(x) if isinstance(x, str) else (0, 0))\n",
    "    idx = ~mask_floor_any\n",
    "    series_floor_tag.loc[idx] = [(0, 0)] * int(idx.sum())\n",
    "    # validate\n",
    "    is_valid_series_floor = series_floor_tag.apply(lambda x: isinstance(x, tuple) and len(x) == 2).all()\n",
    "    if not is_valid_series_floor :\n",
    "        raise ValueError(\"is_valid_series_floor not valid\")\n",
    "    if len(series_floor_tag) == len(series_chambr) == len(series_piece) == len(series_dispo) == len(series_neuf) :\n",
    "        out = df.copy(deep=True)\n",
    "        out[\"DISPONIBL MAINTEN\"] = series_dispo\n",
    "        out[\"NEUF\"] = series_neuf\n",
    "        out[\"CHAMBRE\"] = series_chambr\n",
    "        out[\"PIECE\"] = series_piece\n",
    "        out[[\"FLOOR\",\"FLOORS\"]] = pd.DataFrame(series_floor_tag.tolist(), index=df.index)\n",
    "        return out\n",
    "    else : \n",
    "        raise ValueError(\"len don't match\")#len -> size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "977af84d-bd1e-40bb-964d-dd68c7a56c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- creating linear regression model to predict missing values for CHAMBR,PIECE based on AREA.\n",
    "- why linear regression model ? \n",
    "    + cuz it's num or area is relative to chambre and piece for residential properties,\n",
    "    meaning more area -> more pieces -> more chambres. (that's the relation this code assume)\n",
    "    + we are not going to build the linear regression model from scratch because it's gonna take time\n",
    "    ( i have more thing to do other then sticking my face to the screen all day. \n",
    "        + ps btw i only had less than 1 week to scrape data of seloger.com (got blocked couple times) and k-means applied model training.\n",
    "        + k-means was built from scratch the week before.\n",
    "- we have 3 models, one\n",
    "\"\"\"\n",
    "def round_int(arr):\n",
    "    a = np.asarray(arr, dtype =float)#convert arr to np.array, and cast type to float\n",
    "    return np.round(a).astype(int)#round and convert them to integer\n",
    "def as_float(arr):\n",
    "    return np.asarray(arr, dtype=float)#convert arr to np.array, and cast type to float\n",
    "\n",
    "def make_model(x_cols, alpha=1.0):#just creating model from the lib i imported, to know details, search youtube, or on the website direct.\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy = \"median\")),#fill missing value with median (most appear value)\n",
    "            (\"sc\", StandardScaler()),#standardize features, mean to rescale values by (value-mean)/standard_deviation, standardScaler is the name of the method to scale.\n",
    "        ]), x_cols)\n",
    "    ])\n",
    "    return Pipeline([(\"pre\", pre), (\"reg\", Ridge(alpha=alpha))])\n",
    "#create + train models, LRM -> Linear Regression Model :)\n",
    "def train_LRM(df, alpha_piece=1.0, alpha_ch_direct=1.0, alpha_ch_chain=1.0):#care only about input df, the rest are default settings.\n",
    "    df = df.copy()\n",
    "    for c in [\"AREA\", \"PIECE\", \"CHAMBRE\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors =\"coerce\")\n",
    "\n",
    "    #drop them just in case, most likely when we train the model, we dropped them before.\n",
    "    df_piece = df.dropna(subset = [\"PIECE\"])\n",
    "    df_ch_dir = df.dropna(subset = [\"CHAMBRE\"])\n",
    "    df_ch_chain = df.dropna(subset = [\"CHAMBRE\", \"PIECE\"])\n",
    "\n",
    "    #train models.\n",
    "    m_piece_from_area = make_model([\"AREA\"], alpha = alpha_piece)\n",
    "    m_piece_from_area.fit(df_piece[[\"AREA\"]], df_piece[\"PIECE\"])#to predict piece from area\n",
    "    \n",
    "    m_chambre_from_area = make_model([\"AREA\"], alpha = alpha_ch_direct)\n",
    "    m_chambre_from_area.fit(df_ch_dir[[\"AREA\"]], df_ch_dir[\"CHAMBRE\"])#to predict chambre from area\n",
    "\n",
    "    m_chambre_from_area_piece = make_model([\"AREA\", \"PIECE\"], alpha = alpha_ch_chain)#take area + piece to predict chambre\n",
    "    m_chambre_from_area_piece.fit(df_ch_chain[[\"AREA\", \"PIECE\"]], df_ch_chain[\"CHAMBRE\"])\n",
    "    #return models\n",
    "    models = {\n",
    "        \"piece_from_area\": m_piece_from_area,\n",
    "        \"chambre_from_area\": m_chambre_from_area,\n",
    "        \"chambre_from_area_piece\": m_chambre_from_area_piece,\n",
    "    }\n",
    "    return models\n",
    "#create predicting funcs\n",
    "def predict_A2P(x, models):\n",
    "    try:\n",
    "        X_area = pd.DataFrame([{\"AREA\": float(x)}])\n",
    "        pred = float(models[\"piece_from_area\"].predict(X_area)[0])\n",
    "        return int(round(max(1.0, pred)))\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error predicting AREA to PIECE: {e}\")\n",
    "def predict_A2C(x, models):\n",
    "    try:\n",
    "        X_area = pd.DataFrame([{\"AREA\": float(x)}])\n",
    "        pred = float(models[\"chambre_from_area\"].predict(X_area)[0])\n",
    "        return int(round(max(0.0, pred)))\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error predicting AREA to CHAMBRE: {e}\")\n",
    "def predict_AP2C(area, piece, models):\n",
    "    try:\n",
    "        X_ap = pd.DataFrame([{\"AREA\": float(area), \"PIECE\": float(piece)}])\n",
    "        pred = float(models[\"chambre_from_area_piece\"].predict(X_ap)[0])\n",
    "        pred = max(0.0, min(float(piece), pred))  # 0 ‚â§ CHAMBRE ‚â§ PIECE\n",
    "        return pred\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error predicting (AREA,PIECE) to CHAMBRE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33b4a5c9-2aca-4df7-be55-0d32a9a902c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_predict_features(df, models):\n",
    "    out = df.copy(deep=True)\n",
    "    #create masks\n",
    "    mask_both_missing = (out[\"PIECE\"].isna() & out[\"CHAMBRE\"].isna())\n",
    "    mask_piece_missing = (out[\"PIECE\"].isna() & out[\"CHAMBRE\"].notna())\n",
    "    mask_chambre_missing = (out[\"CHAMBRE\"].isna() & out[\"PIECE\"].notna())\n",
    "    #bool for any missing chambre\n",
    "    if mask_chambre_missing.any():\n",
    "        X_AP = out.loc[mask_chambre_missing, [\"AREA\", \"PIECE\"]].astype(float)\n",
    "        y_hat = models[\"chambre_from_area_piece\"].predict(X_AP).astype(float)\n",
    "        #clip and round\n",
    "        y_hat = np.maximum(0.0, np.minimum(X_AP[\"PIECE\"].to_numpy(), y_hat))\n",
    "        out.loc[mask_chambre_missing, \"CHAMBRE\"] = np.round(y_hat).astype(int)\n",
    "    #cbool for any both chambre and piece missing\n",
    "    if mask_both_missing.any():\n",
    "        #get area vals for rows miss both n convert to float (area is already float type) and convert 1D series to 2D dataframe\n",
    "        area_vals = out.loc[mask_both_missing, \"AREA\"].astype(float).to_frame()\n",
    "        #get prediction\n",
    "        p_hat = models[\"piece_from_area\"].predict(area_vals).astype(float)\n",
    "        p_hat = np.maximum(1.0, p_hat)\n",
    "        p_hat_round = np.round(p_hat).astype(int)\n",
    "        out.loc[mask_both_missing, \"PIECE\"] = p_hat_round\n",
    "\n",
    "        # chambre_hat with chained model\n",
    "        X_for_ch = pd.DataFrame({\n",
    "            \"AREA\": area_vals[\"AREA\"].values,\n",
    "            \"PIECE\": p_hat  # use float p_hat inside model\n",
    "        })\n",
    "        c_hat = models[\"chambre_from_area_piece\"].predict(X_for_ch).astype(float)\n",
    "        c_hat = np.maximum(0.0, np.minimum(p_hat, c_hat))\n",
    "        out.loc[mask_both_missing, \"CHAMBRE\"] = np.round(c_hat).astype(int)\n",
    "\n",
    "    # 3) PIECE missing but CHAMBRE present simply : PIECE = CHAMBRE + 1\n",
    "    if mask_piece_missing.any():\n",
    "        out.loc[mask_piece_missing, \"PIECE\"] = (out.loc[mask_piece_missing, \"CHAMBRE\"].astype(float) + 1).astype(int)\n",
    "    if len(out.loc[out[\"PIECE\"] < out[\"CHAMBRE\"]]) > 0 :\n",
    "        out.loc[out[\"PIECE\"] < out[\"CHAMBRE\"],\"CHAMBRE\"] = out.loc[out[\"PIECE\"] < out[\"CHAMBRE\"],\"PIECE\"]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "646e7ebe-413a-4e22-a6b6-57fa3f1a037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset_seloger(csv_file : str = None, df : pd.DataFrame = None, save : bool = False):\n",
    "    if not csv_file and df is None:\n",
    "        raise ValueError(\"Require dataset filepath or a dataframe\")\n",
    "    #vopen file\n",
    "    if df is not None:\n",
    "        raw_df = df.copy()\n",
    "    else :\n",
    "        try :\n",
    "            raw_df = pd.read_csv(csv_file)\n",
    "        except Exception :\n",
    "            raise ValueError(\"Error converting file.csv to dataframe\")\n",
    "        raw_df_cols = raw_df.columns.tolist()\n",
    "        #verify columns\n",
    "        if raw_df_cols != ['href', 'title']:\n",
    "            raise ValueError(f\"Unexpected columns {raw_df_cols}\")\n",
    "\n",
    "    #1 extract price,unit-price,data-tags -------------------------------\n",
    "    out_df = get_processed(raw_df[\"title\"]) \n",
    "    #2 clean tags (remove numbers, normalize them)\n",
    "    out_df = out_df.loc[out_df[\"DATA TAG\"].notna()]\n",
    "    out_df[\"CLEANED TAG\"] = out_df[\"DATA TAG\"].apply(\n",
    "        lambda x : normalize_tags(x) if isinstance(x,(tuple,list,set)) else pd.NA\n",
    "    )\n",
    "    out_df = out_df.loc[out_df[\"CLEANED TAG\"].notna()]\n",
    "    #3 extract type\n",
    "    out_df[\"TYPE\"] = out_df[\"CLEANED TAG\"].apply(\n",
    "        lambda x : extract_type(x) if isinstance(x,(tuple,list,set)) else pd.NA\n",
    "    )\n",
    "    out_df = out_df.loc[out_df[\"TYPE\"].notna()]\n",
    "\n",
    "    #4 extract location(postcode)\n",
    "    out_df = extract_loc(df=out_df,dict_map=STEM_TO_POSTCODES)\n",
    "    out_df = out_df.loc[out_df[\"LOC\"].notna()]\n",
    "    if out_df[\"LOC\"].apply(lambda x : len(x) == 1 if isinstance(x,(set)) else False).any():\n",
    "        out_df[\"LOC\"] = out_df[\"LOC\"].apply(lambda x : next(iter(x)))\n",
    "    else :\n",
    "        raise ValueError(\"Multiple location in one row detected.\")\n",
    "    #5 extract area,terrain\n",
    "    out_df = extract_area(out_df)\n",
    "    out_df = out_df.loc[~(out_df[\"AREA\"].isna() & out_df[\"TERRAIN\"].isna())]\n",
    "    out_df.loc[out_df[\"TERRAIN\"].isna(),\"TERRAIN\"] = 0\n",
    "    out_df.loc[out_df[\"AREA\"].isna(),\"AREA\"] = 0\n",
    "    out_df[\"AREA\"] = out_df[\"AREA\"].astype(float)\n",
    "    out_df[\"TERRAIN\"] = out_df[\"TERRAIN\"].astype(float)\n",
    "    #6 extract feature\n",
    "    out_df = extract_additionals(out_df)\n",
    "    out_df[\"FLOOR\"] = out_df[\"FLOOR\"].astype(int)\n",
    "    out_df[\"FLOORS\"] = out_df[\"FLOORS\"].astype(int)\n",
    "    #7 fill missing values for chambre and piece, use linear regression model\n",
    "    models = train_LRM(out_df.dropna())\n",
    "    out_df = lin_predict_features(df=out_df,models=models)\n",
    "    out_df[\"CHAMBRE\"] = out_df[\"CHAMBRE\"].astype(int)\n",
    "    out_df[\"PIECE\"] = out_df[\"PIECE\"].astype(int)\n",
    "    out_df.loc[out_df[\"TYPE\"] == \"TERRAIN CONSTRUCTIBL A VENDR\" ,[\"PIECE\",\"CHAMBRE\",\"FLOOR\",\"FLOORS\"]] =0\n",
    "    if save :\n",
    "        out_df.to_csv(f\"v_{len(out_df)}.csv\",index=False)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60e185a5-a0e9-4150-ba20-a28755d9d1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6327"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"dataset/data_sample_1584.csv\")\n",
    "df2 = pd.read_csv(\"dataset/data_sample_4743.csv\")\n",
    "merged_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcfd9e13-822d-4866-8999-efc84d64e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got tag with no digit :  PARIS\n",
      "Got tag with no digit :  PARIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thaim\\AppData\\Local\\Temp\\ipykernel_18612\\3773530752.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mask_has_chambr = df[col_seq].apply(lambda x : has_chambr(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
      "C:\\Users\\thaim\\AppData\\Local\\Temp\\ipykernel_18612\\3773530752.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mask_has_piece = df[col_seq].apply(lambda x : has_piece(list(x)) if isinstance(x,(tuple,list,set)) else pd.NA).fillna(0).astype(bool)\n",
      "C:\\Users\\thaim\\AppData\\Local\\Temp\\ipykernel_18612\\3773530752.py:124: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['ETAG 5/6' 'ETAG 1/7' 'RDC/1' ... 'ETAG 2/5' '2EM ETAG' 'ETAG 3/5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  series_floor_tag.loc[mask_floor_any] = df.loc[mask_has_floor, col_tag].apply(lambda x: get_floor_tag(x) if x else pd.NA)\n"
     ]
    }
   ],
   "source": [
    "vectorized_dataset = vectorize_dataset_seloger(df=merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7e60c-db6a-495d-9315-d3e8a433686c",
   "metadata": {},
   "source": [
    "#### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "98fec72a-6775-4393-8959-29123d55b8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAW                  0\n",
       "DATA TAG             0\n",
       "PRICE                0\n",
       "PRICE_UNIT           0\n",
       "CLEANED TAG          0\n",
       "TYPE                 0\n",
       "LOC                  0\n",
       "ALL AREA             0\n",
       "AREA                 0\n",
       "TERRAIN              0\n",
       "DISPONIBL MAINTEN    0\n",
       "NEUF                 0\n",
       "CHAMBRE              0\n",
       "PIECE                0\n",
       "FLOOR                0\n",
       "FLOORS               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7ba3007-d24b-4359-b1e2-34b6bd67a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RAW', 'DATA TAG', 'PRICE', 'PRICE_UNIT', 'CLEANED TAG', 'TYPE', 'LOC',\n",
       "       'ALL AREA', 'AREA', 'TERRAIN', 'DISPONIBL MAINTEN', 'NEUF', 'CHAMBRE',\n",
       "       'PIECE', 'FLOOR', 'FLOORS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2f7b7509-b516-4b96-844e-d3d8878ee43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DIVISIBL', 'HOTEL PARTICULI A VENDR', 'TERRAIN'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verification for valid tags, there's some tags that's included in the output vectorized_dataset\n",
    "Explanations : \n",
    "- DIVISIBLE was removed from filtering types process\n",
    "- HOTEL PARTICULI A VENDR was later decided not to be used since our models focus on TERRAIN CONSTRUCTIBLE, MAISON, APPART (residential properties)\n",
    "- TERRAIN is extracted to AREA,TERRAIN cols\n",
    "\"\"\"\n",
    "set(vectorized_dataset[\"CLEANED TAG\"].explode().tolist()).symmetric_difference(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c75e252-048d-4eb1-8bfc-e31385434f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APPART A VENDR',\n",
       " 'DUPLEX A VENDR',\n",
       " 'LOFT A VENDR',\n",
       " 'MAISON A VENDR',\n",
       " 'MAISON VILL A VENDR',\n",
       " 'STUDIO A VENDR',\n",
       " 'TERRAIN CONSTRUCTIBL A VENDR',\n",
       " 'VILL A VENDR'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify type\n",
    "set(vectorized_dataset[\"TYPE\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02dafad8-cddf-4310-971e-f44e67f72524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIECE :  15\n",
      "RAW :  Maison √† vendre - Paris 17√®me - 5‚ÄØ100‚ÄØ000¬†‚Ç¨ - 15 pi√®ces, 6 chambres, 500,3 m¬≤ \n",
      " ****************************************************************************************************\n",
      "FLOORS :  35\n",
      "RAW :  Appartement √† vendre - Neuf - Paris 15√®me - 1‚ÄØ150‚ÄØ000¬†‚Ç¨ - 4 pi√®ces, 3 chambres, 93,7 m¬≤, √âtage 16/35 \n",
      " ****************************************************************************************************\n",
      "CHAMBRE :  9\n",
      "RAW :  Maison √† vendre - Paris 16√®me - 7‚ÄØ400‚ÄØ000¬†‚Ç¨ - 12 pi√®ces, 9 chambres, 375 m¬≤ \n",
      " ****************************************************************************************************\n",
      "FLOOR :  34\n",
      "RAW :  Appartement √† vendre - Paris 13√®me - 499‚ÄØ000¬†‚Ç¨ - 4 pi√®ces, 3 chambres, 77 m¬≤, √âtage 34/34 \n",
      " ****************************************************************************************************\n",
      "TERRAIN :  750.0\n",
      "RAW :  Maison √† vendre - Paris 7√®me - 13‚ÄØ500‚ÄØ000¬†‚Ç¨ - 13 pi√®ces, 5 chambres, 300 m¬≤, 750 m¬≤ de terrain \n",
      " ****************************************************************************************************\n",
      "PRICE :  14900000.0\n",
      "RAW :  Appartement √† vendre - Paris 16√®me - 14‚ÄØ900‚ÄØ000¬†‚Ç¨ - 7 pi√®ces, 3 chambres, 280 m¬≤, √âtage 3/6 \n",
      " ****************************************************************************************************\n",
      "AREA :  690.3\n",
      "RAW :  Appartement √† vendre - Paris 14√®me - 5‚ÄØ200‚ÄØ000¬†‚Ç¨ - 8 pi√®ces, 4 chambres, 690,3 m¬≤, RDC/2 \n",
      " ****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#verify cols values\n",
    "def get_max_val(df=vectorized_dataset,col=None):\n",
    "    max_v = vectorized_dataset.loc[vectorized_dataset[col] == max(vectorized_dataset[col])]\n",
    "    print(f\"{col} : \",max_v[col].to_string(index = False))\n",
    "    print(\"RAW : \",max_v[\"RAW\"].iloc[0],\"\\n\",\"*\"*100)\n",
    "check_cols = {\"AREA\",\"TERRAIN\",\"CHAMBRE\",\"PIECE\",\"FLOOR\",\"FLOORS\",\"PRICE\"}\n",
    "for c in check_cols :\n",
    "    get_max_val(col=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "758a0711-7074-46cd-8599-3bece22eeaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIECE :  0\n",
      "RAW :  Terrain constructible √† vendre - Montreuil - 450‚ÄØ000¬†‚Ç¨ - 394 m¬≤ de terrain \n",
      " ****************************************************************************************************\n",
      "FLOORS :  0\n",
      "RAW :  Duplex √† vendre - Neuf - Saint-Cloud - 1‚ÄØ140‚ÄØ000¬†‚Ç¨ - 4 pi√®ces, 3 chambres, 100,8 m¬≤, RDC \n",
      " ****************************************************************************************************\n",
      "CHAMBRE :  0\n",
      "RAW :  Appartement √† vendre - Paris 10√®me - 215‚ÄØ000¬†‚Ç¨ - 1 pi√®ce, 18 m¬≤, RDC/5 \n",
      " ****************************************************************************************************\n",
      "FLOOR :  0\n",
      "RAW :  Appartement √† vendre - Paris 18√®me - 1‚ÄØ550‚ÄØ000¬†‚Ç¨ - 5 pi√®ces, 4 chambres, 191,9 m¬≤, RDC/1 \n",
      " ****************************************************************************************************\n",
      "TERRAIN :  0.0\n",
      "RAW :  Appartement √† vendre - Paris 1er - 560‚ÄØ000¬†‚Ç¨ - 2 pi√®ces, 1 chambre, 45 m¬≤, √âtage 5/6 \n",
      " ****************************************************************************************************\n",
      "PRICE :  49000.0\n",
      "RAW :  Appartement √† vendre - Paris 12√®me - 49‚ÄØ000¬†‚Ç¨ - 1 pi√®ce, 5,5 m¬≤, √âtage 7/7 \n",
      " ****************************************************************************************************\n",
      "AREA :  0.0\n",
      "RAW :  Terrain constructible √† vendre - Montreuil - 450‚ÄØ000¬†‚Ç¨ - 394 m¬≤ de terrain \n",
      " ****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def get_min_val(df=vectorized_dataset,col=None):\n",
    "    max_v = vectorized_dataset.loc[vectorized_dataset[col] == min(vectorized_dataset[col])].iloc[0]\n",
    "    print(f\"{col} : \",max_v[col])\n",
    "    print(\"RAW : \",max_v.iloc[0],\"\\n\",\"*\"*100)\n",
    "#check_cols = {\"AREA\",\"TERRAIN\",\"CHAMBRE\",\"PIECE\",\"FLOOR\",\"FLOORS\",\"PRICE\"}\n",
    "for c in check_cols :\n",
    "    get_min_val(col=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4390de33-730e-43d3-9640-9fa07ee2f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do piece less than chambre ?\n",
    "len(vectorized_dataset.loc[vectorized_dataset[\"PIECE\"] < vectorized_dataset[\"CHAMBRE\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9a0e5875-05df-4aaf-88a8-ae2401a79480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TYPE\n",
       "APPART A VENDR                  4806\n",
       "DUPLEX A VENDR                   234\n",
       "MAISON A VENDR                   119\n",
       "LOFT A VENDR                      22\n",
       "MAISON VILL A VENDR               14\n",
       "STUDIO A VENDR                     7\n",
       "VILL A VENDR                       2\n",
       "TERRAIN CONSTRUCTIBL A VENDR       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataset[\"TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ae93c7f0-49bd-4523-a150-e0a76c411589",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = vectorized_dataset[\"TYPE\"].apply(lambda x : x in [\"APPART A VENDR\",\"DUPLEX A VENDR\",\"MAISON A VENDR\"])\n",
    "vectorized_dataset = vectorized_dataset.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ea61f31f-8e7e-49ce-8d04-62b21924ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seem good, let's save\n",
    "vectorized_dataset.to_csv(f\"v_{len(vectorized_dataset)}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd5697-a54a-4fa0-b33a-866bb5194697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67d9eb-8279-487e-8279-d6be67cda66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
